iMac:Week2 NatarajanShankar$ slcli vs create -d sjc01 --os CENTOS_7 --cpu 2 --memory 4096 --disk 100 --hostname spark1 --domain shankar.net --key RSASL
This action will incur charges on your account. Continue? [y/N]: y
:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 28374845                             :
: created : 2017-02-12T23:55:51-06:00            :
:    guid : 314cca9e-29d7-4997-81d3-412c4dbb4e44 :
:.........:......................................:
iMac:Week2 NatarajanShankar$ slcli vs create -d sjc01 --os CENTOS_7 --cpu 2 --memory 4096 --disk 100 --hostname spark2 --domain shankar.net --key RSASL
This action will incur charges on your account. Continue? [y/N]: y
:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 28374849                             :
: created : 2017-02-12T23:56:12-06:00            :
:    guid : 5a8783eb-6528-41e9-a104-380f2c5ad6db :
:.........:......................................:
iMac:Week2 NatarajanShankar$ slcli vs create -d sjc01 --os CENTOS_7 --cpu 2 --memory 4096 --disk 100 --hostname spark3 --domain shankar.net --key RSASL
This action will incur charges on your account. Continue? [y/N]: y
:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 28374857                             :
: created : 2017-02-12T23:58:20-06:00            :
:    guid : da473398-ac2b-4fb3-85a4-afeeeb8fe467 :
:.........:......................................:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:....................................:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter :               action               :
:..........:..........:................:................:............:....................................:
: 28114889 :  master  : 169.53.128.117 : 10.122.149.204 :   sjc01    :     Reclaim network resources      :
: 28114929 :  slave1  : 169.53.128.118 : 10.122.149.212 :   sjc01    :     Reclaim network resources      :
: 28114947 :  slave2  : 158.85.179.179 : 10.122.149.218 :   sjc01    :        Cloud Server Reclaim        :
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    : Check Cloud Disk Template Transfer :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    : Check Cloud Disk Template Transfer :
: 28374857 :  spark3  :       -        :       -        :     -      :            Assign Host             :
:..........:..........:................:................:............:....................................:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:....................................:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter :               action               :
:..........:..........:................:................:............:....................................:
: 28114889 :  master  : 169.53.128.117 :       -        :   sjc01    :     Reclaim network resources      :
: 28114929 :  slave1  : 169.53.128.118 :       -        :   sjc01    :     Reclaim network resources      :
: 28114947 :  slave2  : 158.85.179.179 :       -        :   sjc01    :     Reclaim network resources      :
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    : Check Cloud Disk Template Transfer :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    : Check Cloud Disk Template Transfer :
: 28374857 :  spark3  :       -        :       -        :     -      :            Assign Host             :
:..........:..........:................:................:............:....................................:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:.................:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter :      action     :
:..........:..........:................:................:............:.................:
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    :        -        :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :        -        :
: 28374857 :  spark3  : 158.85.179.188 : 10.122.149.199 :   sjc01    : Power on server :
:..........:..........:................:................:............:.................:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:........:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter : action :
:..........:..........:................:................:............:........:
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    :   -    :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :   -    :
: 28374857 :  spark3  : 158.85.179.188 : 10.122.149.199 :   sjc01    :   -    :
:..........:..........:................:................:............:........:
iMac:Week2 NatarajanShankar$ slcli vs credentials 28374845
:..........:..........:
: username : password :
:..........:..........:
:   root   : KGC8DHtl :
:..........:..........:
iMac:Week2 NatarajanShankar$ slcli vs credentials 28374849
:..........:..........:
: username : password :
:..........:..........:
:   root   : PGf4NjWw :
:..........:..........:
iMac:Week2 NatarajanShankar$ slcli vs credentials 28374857
:..........:..........:
: username : password :
:..........:..........:
:   root   : Je9D5ny7 :
:..........:..........:
iMac:Week2 NatarajanShankar$ 

iMac:~ NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:........:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter : action :
:..........:..........:................:................:............:........:
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :   -    :
: 28374857 :  spark3  : 158.85.179.188 : 10.122.149.199 :   sjc01    :   -    :
: 28375079 :  spark4  : 169.53.128.114 : 10.122.149.194 :   sjc01    :   -    :
:..........:..........:................:................:............:........:
iMac:~ NatarajanShankar$ slcli vs credentials 28375079
:..........:..........:
: username : password :
:..........:..........:
:   root   : LP7KtHLj :
:..........:..........:
iMac:~ NatarajanShankar$ 






iMac:~ NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:........:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter : action :
:..........:..........:................:................:............:........:
: 28375079 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    :   -    :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :   -    :
: 28374857 :  spark3  : 158.85.179.188 : 10.122.149.199 :   sjc01    :   -    :
:..........:..........:................:................:............:........:
iMac:~ NatarajanShankar$



LATEST


iMac:~ NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:........:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter : action :
:..........:..........:................:................:............:........:
: 28375079 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    :   -    :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :   -    :
: 28375427 :  spark3  : 158.85.179.179 : 10.122.149.199 :   sjc01    :   -    :
:..........:..........:................:................:............:........:
iMac:~ NatarajanShankar$ slcli vs credentials 28375427
:..........:..........:
: username : password :
:..........:..........:
:   root   : L2zetR9k :
:..........:..........:
iMac:~ NatarajanShankar$ 
























iMac:Week2 NatarajanShankar$ slcli vs create -d sjc01 --os CENTOS_7 --cpu 2 --memory 4096 --disk 100 --hostname spark1 --domain shankar.net --key RSASL
This action will incur charges on your account. Continue? [y/N]: y
:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 28374845                             :
: created : 2017-02-12T23:55:51-06:00            :
:    guid : 314cca9e-29d7-4997-81d3-412c4dbb4e44 :
:.........:......................................:
iMac:Week2 NatarajanShankar$ slcli vs create -d sjc01 --os CENTOS_7 --cpu 2 --memory 4096 --disk 100 --hostname spark2 --domain shankar.net --key RSASL
This action will incur charges on your account. Continue? [y/N]: y
:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 28374849                             :
: created : 2017-02-12T23:56:12-06:00            :
:    guid : 5a8783eb-6528-41e9-a104-380f2c5ad6db :
:.........:......................................:
iMac:Week2 NatarajanShankar$ slcli vs create -d sjc01 --os CENTOS_7 --cpu 2 --memory 4096 --disk 100 --hostname spark3 --domain shankar.net --key RSASL
This action will incur charges on your account. Continue? [y/N]: y
:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 28374857                             :
: created : 2017-02-12T23:58:20-06:00            :
:    guid : da473398-ac2b-4fb3-85a4-afeeeb8fe467 :
:.........:......................................:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:....................................:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter :               action               :
:..........:..........:................:................:............:....................................:
: 28114889 :  master  : 169.53.128.117 : 10.122.149.204 :   sjc01    :     Reclaim network resources      :
: 28114929 :  slave1  : 169.53.128.118 : 10.122.149.212 :   sjc01    :     Reclaim network resources      :
: 28114947 :  slave2  : 158.85.179.179 : 10.122.149.218 :   sjc01    :        Cloud Server Reclaim        :
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    : Check Cloud Disk Template Transfer :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    : Check Cloud Disk Template Transfer :
: 28374857 :  spark3  :       -        :       -        :     -      :            Assign Host             :
:..........:..........:................:................:............:....................................:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:....................................:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter :               action               :
:..........:..........:................:................:............:....................................:
: 28114889 :  master  : 169.53.128.117 :       -        :   sjc01    :     Reclaim network resources      :
: 28114929 :  slave1  : 169.53.128.118 :       -        :   sjc01    :     Reclaim network resources      :
: 28114947 :  slave2  : 158.85.179.179 :       -        :   sjc01    :     Reclaim network resources      :
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    : Check Cloud Disk Template Transfer :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    : Check Cloud Disk Template Transfer :
: 28374857 :  spark3  :       -        :       -        :     -      :            Assign Host             :
:..........:..........:................:................:............:....................................:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:.................:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter :      action     :
:..........:..........:................:................:............:.................:
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    :        -        :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :        -        :
: 28374857 :  spark3  : 158.85.179.188 : 10.122.149.199 :   sjc01    : Power on server :
:..........:..........:................:................:............:.................:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:........:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter : action :
:..........:..........:................:................:............:........:
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    :   -    :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :   -    :
: 28374857 :  spark3  : 158.85.179.188 : 10.122.149.199 :   sjc01    :   -    :
:..........:..........:................:................:............:........:
iMac:Week2 NatarajanShankar$ slcli vs credentials 28374845
:..........:..........:
: username : password :
:..........:..........:
:   root   : KGC8DHtl :
:..........:..........:
iMac:Week2 NatarajanShankar$ slcli vs credentials 28374849
:..........:..........:
: username : password :
:..........:..........:
:   root   : PGf4NjWw :
:..........:..........:
iMac:Week2 NatarajanShankar$ slcli vs credentials 28374857
:..........:..........:
: username : password :
:..........:..........:
:   root   : Je9D5ny7 :
:..........:..........:
iMac:Week2 NatarajanShankar$ ssh root@169.53.128.114
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the RSA key sent by the remote host is
SHA256:8SUymDd+nq1TcNpcFTsBvyHJQYMEXKSCh2B/OIEFtao.
Please contact your system administrator.
Add correct host key in /Users/NatarajanShankar/.ssh/known_hosts to get rid of this message.
Offending RSA key in /Users/NatarajanShankar/.ssh/known_hosts:40
RSA host key for 169.53.128.114 has changed and you have requested strict checking.
Host key verification failed.
iMac:Week2 NatarajanShankar$ 
iMac:Week2 NatarajanShankar$ 
iMac:Week2 NatarajanShankar$ ssh root@169.53.128.114
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the RSA key sent by the remote host is
SHA256:8SUymDd+nq1TcNpcFTsBvyHJQYMEXKSCh2B/OIEFtao.
Please contact your system administrator.
Add correct host key in /Users/NatarajanShankar/.ssh/known_hosts to get rid of this message.
Offending RSA key in /Users/NatarajanShankar/.ssh/known_hosts:40
RSA host key for 169.53.128.114 has changed and you have requested strict checking.
Host key verification failed.
iMac:Week2 NatarajanShankar$ ssh root@169.53.128.114
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the RSA key sent by the remote host is
SHA256:8SUymDd+nq1TcNpcFTsBvyHJQYMEXKSCh2B/OIEFtao.
Please contact your system administrator.
Add correct host key in /Users/NatarajanShankar/.ssh/known_hosts to get rid of this message.
Offending RSA key in /Users/NatarajanShankar/.ssh/known_hosts:40
RSA host key for 169.53.128.114 has changed and you have requested strict checking.
Host key verification failed.
iMac:Week2 NatarajanShankar$ slcli vs create -d sjc01 --os CENTOS_7 --cpu 2 --memory 4096 --disk 100 --hostname spark11 --domain shankar.net --key RSASL
This action will incur charges on your account. Continue? [y/N]: y  
:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 28374987                             :
: created : 2017-02-13T00:14:57-06:00            :
:    guid : 561811f9-0942-4cdb-bfe4-787678339968 :
:.........:......................................:
iMac:Week2 NatarajanShankar$ slcli vs credentials
Usage: slcli vs credentials [OPTIONS] IDENTIFIER

Error: Missing argument "identifier".
iMac:Week2 NatarajanShankar$ slcli vs credentials 28374987
An unexpected error has occured:
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/SoftLayer/CLI/core.py", line 163, in main
    cli.main(**kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/click/core.py", line 697, in main
    rv = self.invoke(ctx)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/click/core.py", line 1066, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/click/core.py", line 1066, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/click/core.py", line 895, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/click/core.py", line 535, in invoke
    return callback(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/click/decorators.py", line 64, in new_func
    return ctx.invoke(f, obj, *args[1:], **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/click/core.py", line 535, in invoke
    return callback(*args, **kwargs)
  File "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/SoftLayer/CLI/virt/credentials.py", line 23, in cli
    for item in instance['operatingSystem']['passwords']:
KeyError: 'operatingSystem'

Feel free to report this error as it is likely a bug:
    https://github.com/softlayer/softlayer-python/issues
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:.............:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter :    action   :
:..........:..........:................:................:............:.............:
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    :      -      :
: 28374987 : spark11  :       -        :       -        :     -      : Assign Host :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :      -      :
: 28374857 :  spark3  : 158.85.179.188 : 10.122.149.199 :   sjc01    :      -      :
:..........:..........:................:................:............:.............:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:....................................:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter :               action               :
:..........:..........:................:................:............:....................................:
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    :                 -                  :
: 28374987 : spark11  : 158.85.179.179 : 10.122.149.204 :   sjc01    : Check Cloud Disk Template Transfer :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :                 -                  :
: 28374857 :  spark3  : 158.85.179.188 : 10.122.149.199 :   sjc01    :                 -                  :
:..........:..........:................:................:............:....................................:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:....................................:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter :               action               :
:..........:..........:................:................:............:....................................:
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    :                 -                  :
: 28374987 : spark11  : 158.85.179.179 : 10.122.149.204 :   sjc01    : Check Cloud Disk Template Transfer :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :                 -                  :
: 28374857 :  spark3  : 158.85.179.188 : 10.122.149.199 :   sjc01    :                 -                  :
:..........:..........:................:................:............:....................................:
iMac:Week2 NatarajanShankar$ slcli vs list
:..........:..........:................:................:............:...............................:
:    id    : hostname :   primary_ip   :   backend_ip   : datacenter :             action            :
:..........:..........:................:................:............:...............................:
: 28374845 :  spark1  : 169.53.128.114 : 10.122.149.194 :   sjc01    :   Reclaim network resources   :
: 28374987 : spark11  : 158.85.179.179 : 10.122.149.204 :   sjc01    : Setup provision configuration :
: 28374849 :  spark2  : 158.85.179.184 : 10.122.149.198 :   sjc01    :               -               :
: 28374857 :  spark3  : 158.85.179.188 : 10.122.149.199 :   sjc01    :               -               :
:..........:..........:................:................:............:...............................:
iMac:Week2 NatarajanShankar$ ssh root@158.85.179.179



^C
iMac:Week2 NatarajanShankar$ 
iMac:Week2 NatarajanShankar$ 
iMac:Week2 NatarajanShankar$ ssh root@158.85.179.179
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the ECDSA key sent by the remote host is
SHA256:z5XUyg2xuAjJYtS1EfLgrdWWQvTXQhYLVTT5O7p5Sks.
Please contact your system administrator.
Add correct host key in /Users/NatarajanShankar/.ssh/known_hosts to get rid of this message.
Offending ECDSA key in /Users/NatarajanShankar/.ssh/known_hosts:43
ECDSA host key for 158.85.179.179 has changed and you have requested strict checking.
Host key verification failed.
iMac:Week2 NatarajanShankar$ iMac:Week2 NatarajanShankar$ slcli vs create -d sjc01 --os CENTOS_7 --cpu 2 --memory 4096 --disk 100 --hostname spark4 --domain shankar.net --key RSASL
-bash: iMac:Week2: command not found
iMac:Week2 NatarajanShankar$ slcli vs create -d sjc01 --os CENTOS_7 --cpu 2 --memory 4096 --disk 100 --hostname spark4 --domain shankar.net --key RSASL
This action will incur charges on your account. Continue? [y/N]: y  
:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 28375079                             :
: created : 2017-02-13T00:22:04-06:00            :
:    guid : 5c5f9d32-cc6e-4681-a4fe-13a1ebfb6581 :
:.........:......................................:
iMac:Week2 NatarajanShankar$ ssh root@169.53.128.114
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the RSA key sent by the remote host is
SHA256:PZWU87RcOFHSexF2MHxRkAvM0p2RbOzFejQMxYbdx+8.
Please contact your system administrator.
Add correct host key in /Users/NatarajanShankar/.ssh/known_hosts to get rid of this message.
Offending RSA key in /Users/NatarajanShankar/.ssh/known_hosts:40
RSA host key for 169.53.128.114 has changed and you have requested strict checking.
Host key verification failed.
iMac:Week2 NatarajanShankar$ vi /Users/NatarajanShankar/.ssh/known_hosts
iMac:Week2 NatarajanShankar$ ssh root@169.53.128.114
The authenticity of host '169.53.128.114 (169.53.128.114)' can't be established.
ECDSA key fingerprint is SHA256:RVgUu/rMaSo8oVNYT5EIBgROrsfetUCGjWvRg3eVb2c.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '169.53.128.114' (ECDSA) to the list of known hosts.
root@169.53.128.114's password: 
[root@spark4 ~]# 
[root@spark4 ~]# 
[root@spark4 ~]# 
[root@spark4 ~]# 
[root@spark4 ~]# 
[root@spark4 ~]# 
[root@spark4 ~]# vi /etc/hosts
[root@spark4 ~]# ping spark2
PING spark2.shankar.net (158.85.179.184) 56(84) bytes of data.
64 bytes from spark2.shankar.net (158.85.179.184): icmp_seq=1 ttl=63 time=0.890 ms
64 bytes from spark2.shankar.net (158.85.179.184): icmp_seq=2 ttl=63 time=0.519 ms
64 bytes from spark2.shankar.net (158.85.179.184): icmp_seq=3 ttl=63 time=0.438 ms
^C
--- spark2.shankar.net ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2000ms
rtt min/avg/max/mdev = 0.438/0.615/0.890/0.198 ms
[root@spark4 ~]# ping spark3
PING spark3.shankar.net (158.85.179.188) 56(84) bytes of data.
64 bytes from spark3.shankar.net (158.85.179.188): icmp_seq=1 ttl=63 time=0.931 ms
64 bytes from spark3.shankar.net (158.85.179.188): icmp_seq=2 ttl=63 time=0.492 ms
64 bytes from spark3.shankar.net (158.85.179.188): icmp_seq=3 ttl=63 time=0.503 ms
64 bytes from spark3.shankar.net (158.85.179.188): icmp_seq=4 ttl=63 time=0.542 ms
^C
--- spark3.shankar.net ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3000ms
rtt min/avg/max/mdev = 0.492/0.617/0.931/0.182 ms
[root@spark4 ~]# ssh spark2
The authenticity of host 'spark2 (158.85.179.184)' can't be established.
ECDSA key fingerprint is b9:1f:f9:83:90:5e:cd:5d:49:dd:83:60:7b:07:85:ca.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'spark2,158.85.179.184' (ECDSA) to the list of known hosts.
root@spark2's password: 

[root@spark4 ~]# 
[root@spark4 ~]# 
[root@spark4 ~]# 
[root@spark4 ~]# 
[root@spark4 ~]# cd /root/.ssh
[root@spark4 .ssh]# ls -la
total 16
drwxr-xr-x. 2 root root 4096 Feb 13 00:36 .
dr-xr-x---. 3 root root 4096 Feb 13 00:26 ..
-rw-------. 1 root root  504 Feb 13 00:26 authorized_keys
-rw-r--r--. 1 root root  183 Feb 13 00:36 known_hosts
[root@spark4 .ssh]# vi authorized_keys 
[root@spark4 .ssh]# pwd
/root/.ssh
[root@spark4 .ssh]# !vi
vi authorized_keys 
[root@spark4 .ssh]# vi ~/.ssh/id_rsa
[root@spark4 .ssh]# ssh spark2
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Permissions 0644 for '/root/.ssh/id_rsa' are too open.
It is required that your private key files are NOT accessible by others.
This private key will be ignored.
bad permissions: ignore key: /root/.ssh/id_rsa
root@spark2's password: 

[root@spark4 .ssh]# cd ~/.ssh
[root@spark4 .ssh]# ls -la
total 20
drwxr-xr-x. 2 root root 4096 Feb 13 00:45 .
dr-xr-x---. 3 root root 4096 Feb 13 00:26 ..
-rw-------. 1 root root  504 Feb 13 00:26 authorized_keys
-rw-r--r--. 1 root root 1675 Feb 13 00:45 id_rsa
-rw-r--r--. 1 root root  183 Feb 13 00:36 known_hosts
[root@spark4 .ssh]# chmod 600 id_rsa
[root@spark4 .ssh]# ssh spark2
Last login: Mon Feb 13 00:11:33 2017 from c-73-223-185-251.hsd1.ca.comcast.net
[root@spark2 ~]# 
[root@spark2 ~]# 
[root@spark2 ~]# 
[root@spark2 ~]# ssh spark3
Last login: Mon Feb 13 00:48:08 2017 from spark2.shankar.net
[root@spark3 ~]# 
[root@spark3 ~]# 
[root@spark3 ~]# 
[root@spark3 ~]# 
[root@spark3 ~]# curl https://bintray.com/sbt/rpm/rpm | sudo tee /etc/yum.repos.d/bintray-sbt-rpm.repo
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   162    0   162    0     0    289      0 --:--:-- --:--:-- --:--:--   289
#bintray--sbt-rpm - packages by  from Bintray
[bintray--sbt-rpm]
name=bintray--sbt-rpm
baseurl=http://dl.bintray.com/sbt/rpm
gpgcheck=0
repo_gpgcheck=0
enabled=1
[root@spark3 ~]# yum install -y java-1.8.0-openjdk-headless sbt
Loaded plugins: fastestmirror
base                                                                                                                                               | 3.6 kB  00:00:00     
bintray--sbt-rpm                                                                                                                                   | 1.3 kB  00:00:00     
extras                                                                                                                                             | 3.4 kB  00:00:00     
updates                                                                                                                                            | 3.4 kB  00:00:00     
(1/5): base/7/x86_64/group_gz                                                                                                                      | 155 kB  00:00:00     
(2/5): extras/7/x86_64/primary_db                                                                                                                  | 121 kB  00:00:00     
(3/5): base/7/x86_64/primary_db                                                                                                                    | 5.6 MB  00:00:00     
(4/5): updates/7/x86_64/primary_db                                                                                                                 | 2.2 MB  00:00:00     
(5/5): bintray--sbt-rpm/primary                                                                                                                    | 2.0 kB  00:00:00     
Determining fastest mirrors
bintray--sbt-rpm                                                                                                                                                    13/13
Resolving Dependencies
--> Running transaction check
---> Package java-1.8.0-openjdk-headless.x86_64 1:1.8.0.121-0.b13.el7_3 will be installed
--> Processing Dependency: tzdata-java >= 2015d for package: 1:java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: copy-jdk-configs >= 1.1-3 for package: 1:java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: lksctp-tools(x86-64) for package: 1:java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libjpeg.so.62(LIBJPEG_6.2)(64bit) for package: 1:java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: jpackage-utils for package: 1:java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libjpeg.so.62()(64bit) for package: 1:java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64
---> Package sbt.noarch 0:0.13.13.1-1 will be installed
--> Processing Dependency: java-devel for package: sbt-0.13.13.1-1.noarch
--> Processing Dependency: java for package: sbt-0.13.13.1-1.noarch
--> Running transaction check
---> Package copy-jdk-configs.noarch 0:1.2-1.el7 will be installed
---> Package java-1.8.0-openjdk.x86_64 1:1.8.0.121-0.b13.el7_3 will be installed
--> Processing Dependency: xorg-x11-fonts-Type1 for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libpng15.so.15(PNG15_0)(64bit) for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: fontconfig(x86-64) for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libpng15.so.15()(64bit) for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libgif.so.4()(64bit) for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libXtst.so.6()(64bit) for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libXrender.so.1()(64bit) for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libXi.so.6()(64bit) for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libXext.so.6()(64bit) for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libXcomposite.so.1()(64bit) for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
--> Processing Dependency: libX11.so.6()(64bit) for package: 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64
---> Package java-1.8.0-openjdk-devel.x86_64 1:1.8.0.121-0.b13.el7_3 will be installed
---> Package javapackages-tools.noarch 0:3.4.1-11.el7 will be installed
--> Processing Dependency: python-javapackages = 3.4.1-11.el7 for package: javapackages-tools-3.4.1-11.el7.noarch
--> Processing Dependency: libxslt for package: javapackages-tools-3.4.1-11.el7.noarch
---> Package libjpeg-turbo.x86_64 0:1.2.90-5.el7 will be installed
---> Package lksctp-tools.x86_64 0:1.0.17-2.el7 will be installed
---> Package tzdata-java.noarch 0:2016j-1.el7 will be installed
--> Running transaction check
---> Package fontconfig.x86_64 0:2.10.95-10.el7 will be installed
--> Processing Dependency: fontpackages-filesystem for package: fontconfig-2.10.95-10.el7.x86_64
---> Package giflib.x86_64 0:4.1.6-9.el7 will be installed
--> Processing Dependency: libSM.so.6()(64bit) for package: giflib-4.1.6-9.el7.x86_64
--> Processing Dependency: libICE.so.6()(64bit) for package: giflib-4.1.6-9.el7.x86_64
---> Package libX11.x86_64 0:1.6.3-3.el7 will be installed
--> Processing Dependency: libX11-common >= 1.6.3-3.el7 for package: libX11-1.6.3-3.el7.x86_64
--> Processing Dependency: libxcb.so.1()(64bit) for package: libX11-1.6.3-3.el7.x86_64
---> Package libXcomposite.x86_64 0:0.4.4-4.1.el7 will be installed
---> Package libXext.x86_64 0:1.3.3-3.el7 will be installed
---> Package libXi.x86_64 0:1.7.4-2.el7 will be installed
---> Package libXrender.x86_64 0:0.9.8-2.1.el7 will be installed
---> Package libXtst.x86_64 0:1.2.2-2.1.el7 will be installed
---> Package libpng.x86_64 2:1.5.13-7.el7_2 will be installed
---> Package libxslt.x86_64 0:1.1.28-5.el7 will be installed
---> Package python-javapackages.noarch 0:3.4.1-11.el7 will be installed
--> Processing Dependency: python-lxml for package: python-javapackages-3.4.1-11.el7.noarch
---> Package xorg-x11-fonts-Type1.noarch 0:7.5-9.el7 will be installed
--> Processing Dependency: ttmkfdir for package: xorg-x11-fonts-Type1-7.5-9.el7.noarch
--> Processing Dependency: ttmkfdir for package: xorg-x11-fonts-Type1-7.5-9.el7.noarch
--> Processing Dependency: mkfontdir for package: xorg-x11-fonts-Type1-7.5-9.el7.noarch
--> Processing Dependency: mkfontdir for package: xorg-x11-fonts-Type1-7.5-9.el7.noarch
--> Running transaction check
---> Package fontpackages-filesystem.noarch 0:1.44-8.el7 will be installed
---> Package libICE.x86_64 0:1.0.9-2.el7 will be installed
---> Package libSM.x86_64 0:1.2.2-2.el7 will be installed
---> Package libX11-common.noarch 0:1.6.3-3.el7 will be installed
---> Package libxcb.x86_64 0:1.11-4.el7 will be installed
--> Processing Dependency: libXau.so.6()(64bit) for package: libxcb-1.11-4.el7.x86_64
---> Package python-lxml.x86_64 0:3.2.1-4.el7 will be installed
---> Package ttmkfdir.x86_64 0:3.0.9-42.el7 will be installed
---> Package xorg-x11-font-utils.x86_64 1:7.5-20.el7 will be installed
--> Processing Dependency: libfontenc.so.1()(64bit) for package: 1:xorg-x11-font-utils-7.5-20.el7.x86_64
--> Processing Dependency: libXfont.so.1()(64bit) for package: 1:xorg-x11-font-utils-7.5-20.el7.x86_64
--> Running transaction check
---> Package libXau.x86_64 0:1.0.8-2.1.el7 will be installed
---> Package libXfont.x86_64 0:1.5.1-2.el7 will be installed
---> Package libfontenc.x86_64 0:1.1.2-3.el7 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==========================================================================================================================================================================
 Package                                          Arch                        Version                                         Repository                             Size
==========================================================================================================================================================================
Installing:
 java-1.8.0-openjdk-headless                      x86_64                      1:1.8.0.121-0.b13.el7_3                         updates                                31 M
 sbt                                              noarch                      0.13.13.1-1                                     bintray--sbt-rpm                      1.0 M
Installing for dependencies:
 copy-jdk-configs                                 noarch                      1.2-1.el7                                       base                                   14 k
 fontconfig                                       x86_64                      2.10.95-10.el7                                  base                                  229 k
 fontpackages-filesystem                          noarch                      1.44-8.el7                                      base                                  9.9 k
 giflib                                           x86_64                      4.1.6-9.el7                                     base                                   40 k
 java-1.8.0-openjdk                               x86_64                      1:1.8.0.121-0.b13.el7_3                         updates                               232 k
 java-1.8.0-openjdk-devel                         x86_64                      1:1.8.0.121-0.b13.el7_3                         updates                               9.7 M
 javapackages-tools                               noarch                      3.4.1-11.el7                                    base                                   73 k
 libICE                                           x86_64                      1.0.9-2.el7                                     base                                   65 k
 libSM                                            x86_64                      1.2.2-2.el7                                     base                                   39 k
 libX11                                           x86_64                      1.6.3-3.el7                                     base                                  606 k
 libX11-common                                    noarch                      1.6.3-3.el7                                     base                                  162 k
 libXau                                           x86_64                      1.0.8-2.1.el7                                   base                                   29 k
 libXcomposite                                    x86_64                      0.4.4-4.1.el7                                   base                                   22 k
 libXext                                          x86_64                      1.3.3-3.el7                                     base                                   39 k
 libXfont                                         x86_64                      1.5.1-2.el7                                     base                                  150 k
 libXi                                            x86_64                      1.7.4-2.el7                                     base                                   40 k
 libXrender                                       x86_64                      0.9.8-2.1.el7                                   base                                   25 k
 libXtst                                          x86_64                      1.2.2-2.1.el7                                   base                                   20 k
 libfontenc                                       x86_64                      1.1.2-3.el7                                     base                                   30 k
 libjpeg-turbo                                    x86_64                      1.2.90-5.el7                                    base                                  134 k
 libpng                                           x86_64                      2:1.5.13-7.el7_2                                base                                  213 k
 libxcb                                           x86_64                      1.11-4.el7                                      base                                  189 k
 libxslt                                          x86_64                      1.1.28-5.el7                                    base                                  242 k
 lksctp-tools                                     x86_64                      1.0.17-2.el7                                    base                                   88 k
 python-javapackages                              noarch                      3.4.1-11.el7                                    base                                   31 k
 python-lxml                                      x86_64                      3.2.1-4.el7                                     base                                  758 k
 ttmkfdir                                         x86_64                      3.0.9-42.el7                                    base                                   48 k
 tzdata-java                                      noarch                      2016j-1.el7                                     updates                               182 k
 xorg-x11-font-utils                              x86_64                      1:7.5-20.el7                                    base                                   87 k
 xorg-x11-fonts-Type1                             noarch                      7.5-9.el7                                       base                                  521 k

Transaction Summary
==========================================================================================================================================================================
Install  2 Packages (+30 Dependent packages)

Total download size: 46 M
Installed size: 155 M
Downloading packages:
(1/32): copy-jdk-configs-1.2-1.el7.noarch.rpm                                                                                                      |  14 kB  00:00:00     
(2/32): fontpackages-filesystem-1.44-8.el7.noarch.rpm                                                                                              | 9.9 kB  00:00:00     
(3/32): giflib-4.1.6-9.el7.x86_64.rpm                                                                                                              |  40 kB  00:00:00     
(4/32): fontconfig-2.10.95-10.el7.x86_64.rpm                                                                                                       | 229 kB  00:00:00     
(5/32): java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64.rpm                                                                                        | 232 kB  00:00:00     
(6/32): javapackages-tools-3.4.1-11.el7.noarch.rpm                                                                                                 |  73 kB  00:00:00     
(7/32): libSM-1.2.2-2.el7.x86_64.rpm                                                                                                               |  39 kB  00:00:00     
(8/32): libICE-1.0.9-2.el7.x86_64.rpm                                                                                                              |  65 kB  00:00:00     
(9/32): libX11-1.6.3-3.el7.x86_64.rpm                                                                                                              | 606 kB  00:00:00     
(10/32): libXau-1.0.8-2.1.el7.x86_64.rpm                                                                                                           |  29 kB  00:00:00     
(11/32): libXcomposite-0.4.4-4.1.el7.x86_64.rpm                                                                                                    |  22 kB  00:00:00     
(12/32): libX11-common-1.6.3-3.el7.noarch.rpm                                                                                                      | 162 kB  00:00:00     
(13/32): libXext-1.3.3-3.el7.x86_64.rpm                                                                                                            |  39 kB  00:00:00     
(14/32): libXfont-1.5.1-2.el7.x86_64.rpm                                                                                                           | 150 kB  00:00:00     
(15/32): libXi-1.7.4-2.el7.x86_64.rpm                                                                                                              |  40 kB  00:00:00     
(16/32): java-1.8.0-openjdk-devel-1.8.0.121-0.b13.el7_3.x86_64.rpm                                                                                 | 9.7 MB  00:00:00     
(17/32): libXrender-0.9.8-2.1.el7.x86_64.rpm                                                                                                       |  25 kB  00:00:00     
(18/32): libXtst-1.2.2-2.1.el7.x86_64.rpm                                                                                                          |  20 kB  00:00:00     
(19/32): libfontenc-1.1.2-3.el7.x86_64.rpm                                                                                                         |  30 kB  00:00:00     
(20/32): libpng-1.5.13-7.el7_2.x86_64.rpm                                                                                                          | 213 kB  00:00:00     
(21/32): libxcb-1.11-4.el7.x86_64.rpm                                                                                                              | 189 kB  00:00:00     
(22/32): libxslt-1.1.28-5.el7.x86_64.rpm                                                                                                           | 242 kB  00:00:00     
(23/32): lksctp-tools-1.0.17-2.el7.x86_64.rpm                                                                                                      |  88 kB  00:00:00     
(24/32): libjpeg-turbo-1.2.90-5.el7.x86_64.rpm                                                                                                     | 134 kB  00:00:00     
(25/32): python-javapackages-3.4.1-11.el7.noarch.rpm                                                                                               |  31 kB  00:00:00     
(26/32): python-lxml-3.2.1-4.el7.x86_64.rpm                                                                                                        | 758 kB  00:00:00     
(27/32): ttmkfdir-3.0.9-42.el7.x86_64.rpm                                                                                                          |  48 kB  00:00:00     
(28/32): java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64.rpm                                                                              |  31 MB  00:00:00     
(29/32): xorg-x11-font-utils-7.5-20.el7.x86_64.rpm                                                                                                 |  87 kB  00:00:00     
(30/32): tzdata-java-2016j-1.el7.noarch.rpm                                                                                                        | 182 kB  00:00:00     
(31/32): xorg-x11-fonts-Type1-7.5-9.el7.noarch.rpm                                                                                                 | 521 kB  00:00:00     
(32/32): sbt-0.13.13.1.rpm                                                                                                                         | 1.0 MB  00:00:00     
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total                                                                                                                                      50 MB/s |  46 MB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : libfontenc-1.1.2-3.el7.x86_64                                                                                                                         1/32 
  Installing : libICE-1.0.9-2.el7.x86_64                                                                                                                             2/32 
  Installing : libxslt-1.1.28-5.el7.x86_64                                                                                                                           3/32 
  Installing : libjpeg-turbo-1.2.90-5.el7.x86_64                                                                                                                     4/32 
  Installing : python-lxml-3.2.1-4.el7.x86_64                                                                                                                        5/32 
  Installing : python-javapackages-3.4.1-11.el7.noarch                                                                                                               6/32 
  Installing : javapackages-tools-3.4.1-11.el7.noarch                                                                                                                7/32 
  Installing : libSM-1.2.2-2.el7.x86_64                                                                                                                              8/32 
  Installing : libXfont-1.5.1-2.el7.x86_64                                                                                                                           9/32 
  Installing : 1:xorg-x11-font-utils-7.5-20.el7.x86_64                                                                                                              10/32 
  Installing : libX11-common-1.6.3-3.el7.noarch                                                                                                                     11/32 
  Installing : libXau-1.0.8-2.1.el7.x86_64                                                                                                                          12/32 
  Installing : libxcb-1.11-4.el7.x86_64                                                                                                                             13/32 
  Installing : libX11-1.6.3-3.el7.x86_64                                                                                                                            14/32 
  Installing : libXext-1.3.3-3.el7.x86_64                                                                                                                           15/32 
  Installing : libXi-1.7.4-2.el7.x86_64                                                                                                                             16/32 
  Installing : libXtst-1.2.2-2.1.el7.x86_64                                                                                                                         17/32 
  Installing : giflib-4.1.6-9.el7.x86_64                                                                                                                            18/32 
  Installing : libXcomposite-0.4.4-4.1.el7.x86_64                                                                                                                   19/32 
  Installing : libXrender-0.9.8-2.1.el7.x86_64                                                                                                                      20/32 
  Installing : tzdata-java-2016j-1.el7.noarch                                                                                                                       21/32 
  Installing : copy-jdk-configs-1.2-1.el7.noarch                                                                                                                    22/32 
  Installing : lksctp-tools-1.0.17-2.el7.x86_64                                                                                                                     23/32 
  Installing : 1:java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64                                                                                           24/32 
  Installing : fontpackages-filesystem-1.44-8.el7.noarch                                                                                                            25/32 
  Installing : fontconfig-2.10.95-10.el7.x86_64                                                                                                                     26/32 
  Installing : ttmkfdir-3.0.9-42.el7.x86_64                                                                                                                         27/32 
  Installing : xorg-x11-fonts-Type1-7.5-9.el7.noarch                                                                                                                28/32 
  Installing : 2:libpng-1.5.13-7.el7_2.x86_64                                                                                                                       29/32 
  Installing : 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64                                                                                                    30/32 
  Installing : 1:java-1.8.0-openjdk-devel-1.8.0.121-0.b13.el7_3.x86_64                                                                                              31/32 
  Installing : sbt-0.13.13.1-1.noarch                                                                                                                               32/32 
  Verifying  : libXext-1.3.3-3.el7.x86_64                                                                                                                            1/32 
  Verifying  : 1:java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64                                                                                            2/32 
  Verifying  : giflib-4.1.6-9.el7.x86_64                                                                                                                             3/32 
  Verifying  : libjpeg-turbo-1.2.90-5.el7.x86_64                                                                                                                     4/32 
  Verifying  : libXtst-1.2.2-2.1.el7.x86_64                                                                                                                          5/32 
  Verifying  : python-lxml-3.2.1-4.el7.x86_64                                                                                                                        6/32 
  Verifying  : libxcb-1.11-4.el7.x86_64                                                                                                                              7/32 
  Verifying  : 2:libpng-1.5.13-7.el7_2.x86_64                                                                                                                        8/32 
  Verifying  : 1:java-1.8.0-openjdk-devel-1.8.0.121-0.b13.el7_3.x86_64                                                                                               9/32 
  Verifying  : ttmkfdir-3.0.9-42.el7.x86_64                                                                                                                         10/32 
  Verifying  : fontpackages-filesystem-1.44-8.el7.noarch                                                                                                            11/32 
  Verifying  : python-javapackages-3.4.1-11.el7.noarch                                                                                                              12/32 
  Verifying  : libXcomposite-0.4.4-4.1.el7.x86_64                                                                                                                   13/32 
  Verifying  : sbt-0.13.13.1-1.noarch                                                                                                                               14/32 
  Verifying  : libXrender-0.9.8-2.1.el7.x86_64                                                                                                                      15/32 
  Verifying  : lksctp-tools-1.0.17-2.el7.x86_64                                                                                                                     16/32 
  Verifying  : copy-jdk-configs-1.2-1.el7.noarch                                                                                                                    17/32 
  Verifying  : tzdata-java-2016j-1.el7.noarch                                                                                                                       18/32 
  Verifying  : xorg-x11-fonts-Type1-7.5-9.el7.noarch                                                                                                                19/32 
  Verifying  : libxslt-1.1.28-5.el7.x86_64                                                                                                                          20/32 
  Verifying  : libXfont-1.5.1-2.el7.x86_64                                                                                                                          21/32 
  Verifying  : libICE-1.0.9-2.el7.x86_64                                                                                                                            22/32 
  Verifying  : javapackages-tools-3.4.1-11.el7.noarch                                                                                                               23/32 
  Verifying  : libXi-1.7.4-2.el7.x86_64                                                                                                                             24/32 
  Verifying  : libXau-1.0.8-2.1.el7.x86_64                                                                                                                          25/32 
  Verifying  : libSM-1.2.2-2.el7.x86_64                                                                                                                             26/32 
  Verifying  : libfontenc-1.1.2-3.el7.x86_64                                                                                                                        27/32 
  Verifying  : libX11-1.6.3-3.el7.x86_64                                                                                                                            28/32 
  Verifying  : 1:java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64                                                                                                    29/32 
  Verifying  : libX11-common-1.6.3-3.el7.noarch                                                                                                                     30/32 
  Verifying  : fontconfig-2.10.95-10.el7.x86_64                                                                                                                     31/32 
  Verifying  : 1:xorg-x11-font-utils-7.5-20.el7.x86_64                                                                                                              32/32 

Installed:
  java-1.8.0-openjdk-headless.x86_64 1:1.8.0.121-0.b13.el7_3                                           sbt.noarch 0:0.13.13.1-1                                          

Dependency Installed:
  copy-jdk-configs.noarch 0:1.2-1.el7              fontconfig.x86_64 0:2.10.95-10.el7                       fontpackages-filesystem.noarch 0:1.44-8.el7                   
  giflib.x86_64 0:4.1.6-9.el7                      java-1.8.0-openjdk.x86_64 1:1.8.0.121-0.b13.el7_3        java-1.8.0-openjdk-devel.x86_64 1:1.8.0.121-0.b13.el7_3       
  javapackages-tools.noarch 0:3.4.1-11.el7         libICE.x86_64 0:1.0.9-2.el7                              libSM.x86_64 0:1.2.2-2.el7                                    
  libX11.x86_64 0:1.6.3-3.el7                      libX11-common.noarch 0:1.6.3-3.el7                       libXau.x86_64 0:1.0.8-2.1.el7                                 
  libXcomposite.x86_64 0:0.4.4-4.1.el7             libXext.x86_64 0:1.3.3-3.el7                             libXfont.x86_64 0:1.5.1-2.el7                                 
  libXi.x86_64 0:1.7.4-2.el7                       libXrender.x86_64 0:0.9.8-2.1.el7                        libXtst.x86_64 0:1.2.2-2.1.el7                                
  libfontenc.x86_64 0:1.1.2-3.el7                  libjpeg-turbo.x86_64 0:1.2.90-5.el7                      libpng.x86_64 2:1.5.13-7.el7_2                                
  libxcb.x86_64 0:1.11-4.el7                       libxslt.x86_64 0:1.1.28-5.el7                            lksctp-tools.x86_64 0:1.0.17-2.el7                            
  python-javapackages.noarch 0:3.4.1-11.el7        python-lxml.x86_64 0:3.2.1-4.el7                         ttmkfdir.x86_64 0:3.0.9-42.el7                                
  tzdata-java.noarch 0:2016j-1.el7                 xorg-x11-font-utils.x86_64 1:7.5-20.el7                  xorg-x11-fonts-Type1.noarch 0:7.5-9.el7                       

Complete!
[root@spark3 ~]# Connection to spark3 closed by remote host.
Connection to spark3 closed.
[root@spark2 ~]# vi /etc/hosts
[root@spark2 ~]# vi /etc/hosts
[root@spark2 ~]# pring spark2
-bash: pring: command not found
[root@spark2 ~]# ping spark2
PING spark2.shankar.net (158.85.179.184) 56(84) bytes of data.
64 bytes from spark2.shankar.net (158.85.179.184): icmp_seq=1 ttl=64 time=0.027 ms
64 bytes from spark2.shankar.net (158.85.179.184): icmp_seq=2 ttl=64 time=0.022 ms
64 bytes from spark2.shankar.net (158.85.179.184): icmp_seq=3 ttl=64 time=0.018 ms
^C
--- spark2.shankar.net ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2000ms
rtt min/avg/max/mdev = 0.018/0.022/0.027/0.005 ms
[root@spark2 ~]# ping spark3
PING spark3.shankar.net (158.85.179.179) 56(84) bytes of data.
64 bytes from spark3.shankar.net (158.85.179.179): icmp_seq=1 ttl=64 time=1.01 ms
64 bytes from spark3.shankar.net (158.85.179.179): icmp_seq=2 ttl=64 time=0.431 ms
64 bytes from spark3.shankar.net (158.85.179.179): icmp_seq=3 ttl=64 time=0.671 ms
64 bytes from spark3.shankar.net (158.85.179.179): icmp_seq=4 ttl=64 time=0.442 ms
^C
--- spark3.shankar.net ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3000ms
rtt min/avg/max/mdev = 0.431/0.640/1.019/0.240 ms
[root@spark2 ~]# vi /etc/hosta
[root@spark2 ~]# vi /etc/hosts
[root@spark2 ~]# exit
logout
Connection to spark2 closed.
[root@spark4 .ssh]# 
[root@spark4 .ssh]# 
[root@spark4 .ssh]# 
[root@spark4 .ssh]# 
[root@spark4 .ssh]# 
[root@spark4 .ssh]# 
[root@spark4 .ssh]# curl http://www.gtlib.gatech.edu/pub/apache/spark/spark-1.6.2/spark-1.6.2-bin-hadoop2.6.tgz | tar -zx -C /usr/local --show-transformed --transform='s,/*[^/]*,spark,'
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  265M  100  265M    0     0  13.7M      0  0:00:19  0:00:19 --:--:-- 10.6M
[root@spark4 .ssh]# echo export SPARK_HOME=\"/usr/local/spark\" >> /root/.bash_profile
[root@spark4 .ssh]# source /root/.bash_profile
[root@spark4 .ssh]# vi $SPARK_HOME/conf/slaves
[root@spark4 .ssh]# echo $SPARK_BIN

[root@spark4 .ssh]# echo $SPARK_HOME
/usr/local/spark
[root@spark4 .ssh]# cd /usr/local/spark/sbin
[root@spark4 sbin]# ls -la
total 100
drwxr-xr-x.  2 500 500 4096 Jun 21  2016 .
drwxr-xr-x. 12 500 500 4096 Jun 21  2016 ..
-rwxr-xr-x.  1 500 500 2803 Jun 21  2016 slaves.sh
-rwxr-xr-x.  1 500 500 1341 Jun 21  2016 spark-config.sh
-rwxr-xr-x.  1 500 500 5243 Jun 21  2016 spark-daemon.sh
-rwxr-xr-x.  1 500 500 1262 Jun 21  2016 spark-daemons.sh
-rwxr-xr-x.  1 500 500 1347 Jun 21  2016 start-all.sh
-rwxr-xr-x.  1 500 500 1272 Jun 21  2016 start-history-server.sh
-rwxr-xr-x.  1 500 500 2403 Jun 21  2016 start-master.sh
-rwxr-xr-x.  1 500 500 1622 Jun 21  2016 start-mesos-dispatcher.sh
-rwxr-xr-x.  1 500 500 1423 Jun 21  2016 start-mesos-shuffle-service.sh
-rwxr-xr-x.  1 500 500 1279 Jun 21  2016 start-shuffle-service.sh
-rwxr-xr-x.  1 500 500 3151 Jun 21  2016 start-slave.sh
-rwxr-xr-x.  1 500 500 2061 Jun 21  2016 start-slaves.sh
-rwxr-xr-x.  1 500 500 1824 Jun 21  2016 start-thriftserver.sh
-rwxr-xr-x.  1 500 500 1478 Jun 21  2016 stop-all.sh
-rwxr-xr-x.  1 500 500 1056 Jun 21  2016 stop-history-server.sh
-rwxr-xr-x.  1 500 500 1220 Jun 21  2016 stop-master.sh
-rwxr-xr-x.  1 500 500 1112 Jun 21  2016 stop-mesos-dispatcher.sh
-rwxr-xr-x.  1 500 500 1084 Jun 21  2016 stop-mesos-shuffle-service.sh
-rwxr-xr-x.  1 500 500 1067 Jun 21  2016 stop-shuffle-service.sh
-rwxr-xr-x.  1 500 500 1557 Jun 21  2016 stop-slave.sh
-rwxr-xr-x.  1 500 500 1298 Jun 21  2016 stop-slaves.sh
-rwxr-xr-x.  1 500 500 1066 Jun 21  2016 stop-thriftserver.sh
[root@spark4 sbin]# $SPARK_HOME/sbin/start-master.sh
starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-spark4.shankar.net.out
[root@spark4 sbin]# 
[root@spark4 sbin]# 
[root@spark4 sbin]# $SPARK_HOME/sbin/start-slaves.sh
spark1: Warning: Permanently added 'spark1,169.53.128.114' (ECDSA) to the list of known hosts.
spark1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark4.shankar.net.out
spark2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark2.shankar.net.out
spark1: failed to launch org.apache.spark.deploy.worker.Worker:
spark1:   	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
spark1:   	at java.lang.Thread.run(Thread.java:745)
spark1: full log in /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark4.shankar.net.out




spark3: ssh: connect to host spark3 port 22: Connection timed out
[root@spark4 sbin]# 
[root@spark4 sbin]# 
[root@spark4 sbin]# 
[root@spark4 sbin]# 
[root@spark4 sbin]# hostname
spark4.shankar.net
[root@spark4 sbin]# hostname spark1.shankar.net
[root@spark4 sbin]# hostname
spark1.shankar.net
[root@spark4 sbin]# $SPARK_HOME/sbin/start-slaves.sh
spark1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark1.shankar.net.out
spark2: org.apache.spark.deploy.worker.Worker running as process 19545.  Stop it first.



^C[root@spark4 sbin]# $SPARK_HOME/sbin/start-slaves.sh
spark1: org.apache.spark.deploy.worker.Worker running as process 11920.  Stop it first.
spark2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark2.shankar.net.out
^C[root@spark4 sbin]# kill -9 11920
[root@spark4 sbin]# spark3: ssh: connect to host spark3 port 22: Connection timed out

[root@spark4 sbin]# 
[root@spark4 sbin]# 
[root@spark4 sbin]# 
[root@spark4 sbin]# 
[root@spark4 sbin]# 
[root@spark4 sbin]# $SPARK_HOME/sbin/start-slaves.sh
spark1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark1.shankar.net.out
spark2: org.apache.spark.deploy.worker.Worker running as process 19813.  Stop it first.
^C[root@spark4 sbin]# spark3: ssh: connect to host spark3 port 22: Connection timed out

[root@spark4 sbin]# 
[root@spark4 sbin]# 
[root@spark4 sbin]# ssh spark3

^C
[root@spark4 sbin]# 
[root@spark4 sbin]# 
[root@spark4 sbin]# vi /etc/hospark3: ssh: connect to host spark3 port 22: Connection timed out
sts
[root@spark4 sbin]# ping spark3
PING spark3.shankar.net (158.85.179.179) 56(84) bytes of data.
64 bytes from spark3.shankar.net (158.85.179.179): icmp_seq=1 ttl=63 time=0.816 ms
64 bytes from spark3.shankar.net (158.85.179.179): icmp_seq=2 ttl=63 time=0.463 ms
64 bytes from spark3.shankar.net (158.85.179.179): icmp_seq=3 ttl=63 time=0.461 ms
^C
--- spark3.shankar.net ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2000ms
rtt min/avg/max/mdev = 0.461/0.580/0.816/0.166 ms
[root@spark4 sbin]# ps -aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.1  46164  6640 ?        Ss   00:26   0:01 /usr/lib/systemd/systemd --switched-root --system --deserialize 21
root         2  0.0  0.0      0     0 ?        S    00:26   0:00 [kthreadd]
root         3  0.0  0.0      0     0 ?        S    00:26   0:00 [ksoftirqd/0]
root         5  0.0  0.0      0     0 ?        S<   00:26   0:00 [kworker/0:0H]
root         6  0.0  0.0      0     0 ?        S    00:26   0:00 [kworker/u30:0]
root         7  0.0  0.0      0     0 ?        S    00:26   0:00 [migration/0]
root         8  0.0  0.0      0     0 ?        S    00:26   0:00 [rcu_bh]
root         9  0.0  0.0      0     0 ?        S    00:26   0:00 [rcu_sched]
root        10  0.0  0.0      0     0 ?        S    00:26   0:00 [watchdog/0]
root        11  0.0  0.0      0     0 ?        S    00:26   0:00 [watchdog/1]
root        12  0.0  0.0      0     0 ?        S    00:26   0:00 [migration/1]
root        13  0.0  0.0      0     0 ?        S    00:26   0:00 [ksoftirqd/1]
root        15  0.0  0.0      0     0 ?        S<   00:26   0:00 [kworker/1:0H]
root        17  0.0  0.0      0     0 ?        S<   00:26   0:00 [khelper]
root        18  0.0  0.0      0     0 ?        S    00:26   0:00 [kdevtmpfs]
root        19  0.0  0.0      0     0 ?        S<   00:26   0:00 [netns]
root        20  0.0  0.0      0     0 ?        S    00:26   0:00 [xenwatch]
root        21  0.0  0.0      0     0 ?        S    00:26   0:00 [xenbus]
root        22  0.0  0.0      0     0 ?        R    00:26   0:00 [kworker/0:1]
root        23  0.0  0.0      0     0 ?        S    00:26   0:00 [khungtaskd]
root        24  0.0  0.0      0     0 ?        S<   00:26   0:00 [writeback]
root        25  0.0  0.0      0     0 ?        S<   00:26   0:00 [kintegrityd]
root        26  0.0  0.0      0     0 ?        S<   00:26   0:00 [bioset]
root        27  0.0  0.0      0     0 ?        S<   00:26   0:00 [kblockd]
root        28  0.0  0.0      0     0 ?        S    00:26   0:00 [kworker/1:1]
root        29  0.0  0.0      0     0 ?        S<   00:26   0:00 [md]
root        34  0.0  0.0      0     0 ?        S    00:26   0:00 [kswapd0]
root        35  0.0  0.0      0     0 ?        SN   00:26   0:00 [ksmd]
root        36  0.0  0.0      0     0 ?        SN   00:26   0:00 [khugepaged]
root        37  0.0  0.0      0     0 ?        S    00:26   0:00 [fsnotify_mark]
root        38  0.0  0.0      0     0 ?        S<   00:26   0:00 [crypto]
root        46  0.0  0.0      0     0 ?        S<   00:26   0:00 [kthrotld]
root        47  0.0  0.0      0     0 ?        S    00:26   0:00 [kworker/u30:1]
root        48  0.0  0.0      0     0 ?        S    00:26   0:00 [khvcd]
root        49  0.0  0.0      0     0 ?        S<   00:26   0:00 [kmpath_rdacd]
root        50  0.0  0.0      0     0 ?        S<   00:26   0:00 [kpsmoused]
root        52  0.0  0.0      0     0 ?        S<   00:26   0:00 [ipv6_addrconf]
root        71  0.0  0.0      0     0 ?        S<   00:26   0:00 [deferwq]
root       111  0.0  0.0      0     0 ?        S    00:26   0:00 [kworker/1:2]
root       155  0.0  0.0      0     0 ?        S    00:26   0:00 [kauditd]
root       235  0.0  0.0      0     0 ?        S<   00:26   0:00 [ata_sff]
root       237  0.0  0.0      0     0 ?        S    00:26   0:00 [scsi_eh_0]
root       239  0.0  0.0      0     0 ?        S<   00:26   0:00 [scsi_tmf_0]
root       240  0.0  0.0      0     0 ?        S    00:26   0:00 [scsi_eh_1]
root       242  0.0  0.0      0     0 ?        S<   00:26   0:00 [scsi_tmf_1]
root       250  0.0  0.0      0     0 ?        S<   00:26   0:00 [ttm_swap]
root       277  0.0  0.0      0     0 ?        S    00:26   0:00 [jbd2/xvda2-8]
root       278  0.0  0.0      0     0 ?        S<   00:26   0:00 [ext4-rsv-conver]
root       351  0.0  0.0  36816  3260 ?        Ss   00:26   0:00 /usr/lib/systemd/systemd-journald
root       368  0.0  0.1  46444  4836 ?        Ss   00:26   0:00 /usr/lib/systemd/systemd-udevd
root       450  0.0  0.0      0     0 ?        S    00:26   0:00 [jbd2/xvda1-8]
root       451  0.0  0.0      0     0 ?        S<   00:26   0:00 [ext4-rsv-conver]
root       470  0.0  0.0  55416  1720 ?        S<sl 00:26   0:00 /sbin/auditd -n
root       492  0.0  0.0  19168  1208 ?        Ss   00:26   0:00 /usr/sbin/irqbalance --foreground
root       495  0.0  0.1 219776  3704 ?        Ssl  00:26   0:00 /usr/sbin/rsyslogd -n
polkitd    497  0.0  0.2 527516 10828 ?        Ssl  00:26   0:00 /usr/lib/polkit-1/polkitd --no-debug
avahi      498  0.0  0.0  28112  1512 ?        Ss   00:26   0:00 avahi-daemon: running [spark4.local]
root       502  0.0  0.4 553152 16440 ?        Ssl  00:26   0:00 /usr/bin/python -Es /usr/sbin/tuned -l -P
dbus       503  0.0  0.0  32704  1844 ?        Ssl  00:26   0:00 /bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation
avahi      538  0.0  0.0  27980   232 ?        S    00:26   0:00 avahi-daemon: chroot helper
root       547  0.0  0.0  24192  1676 ?        Ss   00:26   0:00 /usr/lib/systemd/systemd-logind
root       552  0.0  0.0 126220  1668 ?        Ss   00:26   0:00 /usr/sbin/crond -n
root       566  0.0  0.0 110036   848 tty1     Ss+  00:26   0:00 /sbin/agetty --noclear tty1 linux
root       567  0.0  0.0 110036   852 hvc0     Ss+  00:26   0:00 /sbin/agetty --keep-baud 115200 38400 9600 hvc0 vt220
root       572  0.0  0.0      0     0 ?        S<   00:26   0:00 [kworker/0:1H]
root       648  0.0  0.0  82468  1336 ?        Ss   00:26   0:00 /usr/sbin/sshd
root       824  0.0  0.0  88980  2080 ?        Ss   00:26   0:00 /usr/libexec/postfix/master -w
postfix    834  0.0  0.1  89084  3936 ?        S    00:26   0:00 pickup -l -t unix -u
postfix    835  0.0  0.1  89152  3952 ?        S    00:26   0:00 qmgr -l -t unix -u
root      1118  0.0  0.0  11632  1484 ?        S    00:26   0:00 /bin/bash /usr/sbin/xe-daemon -p /var/run/xe-daemon.pid
root      8124  0.0  0.0      0     0 ?        S<   00:26   0:00 [kworker/1:1H]
root      9477  0.0  0.1 142904  5112 ?        Ss   00:28   0:00 sshd: root@pts/0
root      9530  0.0  0.0 115384  2080 pts/0    Ss   00:29   0:00 -bash
root     10936  0.0  0.0 123196   732 ?        Ss   01:01   0:00 /usr/sbin/anacron -s
root     11765  0.0  0.0      0     0 ?        S    01:13   0:00 [kworker/0:0]
root     12183  1.8  5.3 3648304 192612 ?      Sl   01:19   0:03 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64/jre/bin/java -cp /usr/local/spark/conf/:/usr
root     12283  0.2  0.1 142904  5132 ?        Ss   01:20   0:00 sshd: root@pts/1
root     12286  0.0  0.0 115384  1996 pts/1    Ss+  01:20   0:00 -bash
root     12384  0.0  0.0   4308   344 ?        S    01:21   0:00 sleep 60
root     12386  0.0  0.0 151056  1828 pts/0    R+   01:21   0:00 ps -aux
[root@spark4 sbin]# kill -9 12183
[root@spark4 sbin]# $SPARK_HOME/sbin/start-slaves.sh
spark3: Warning: Permanently added 'spark3,158.85.179.179' (ECDSA) to the list of known hosts.
spark3: bash: line 0: cd: /usr/local/spark: No such file or directory
spark3: bash: /usr/local/spark/sbin/start-slave.sh: No such file or directory
spark1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark1.shankar.net.out
spark2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark2.shankar.net.out
[root@spark4 sbin]# ps -aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.1  46164  6640 ?        Ss   00:26   0:01 /usr/lib/systemd/systemd --switched-root --system --deserialize 21
root         2  0.0  0.0      0     0 ?        S    00:26   0:00 [kthreadd]
root         3  0.0  0.0      0     0 ?        S    00:26   0:00 [ksoftirqd/0]
root         5  0.0  0.0      0     0 ?        S<   00:26   0:00 [kworker/0:0H]
root         6  0.0  0.0      0     0 ?        S    00:26   0:00 [kworker/u30:0]
root         7  0.0  0.0      0     0 ?        S    00:26   0:00 [migration/0]
root         8  0.0  0.0      0     0 ?        S    00:26   0:00 [rcu_bh]
root         9  0.0  0.0      0     0 ?        S    00:26   0:00 [rcu_sched]
root        10  0.0  0.0      0     0 ?        S    00:26   0:00 [watchdog/0]
root        11  0.0  0.0      0     0 ?        S    00:26   0:00 [watchdog/1]
root        12  0.0  0.0      0     0 ?        S    00:26   0:00 [migration/1]
root        13  0.0  0.0      0     0 ?        S    00:26   0:00 [ksoftirqd/1]
root        15  0.0  0.0      0     0 ?        S<   00:26   0:00 [kworker/1:0H]
root        17  0.0  0.0      0     0 ?        S<   00:26   0:00 [khelper]
root        18  0.0  0.0      0     0 ?        S    00:26   0:00 [kdevtmpfs]
root        19  0.0  0.0      0     0 ?        S<   00:26   0:00 [netns]
root        20  0.0  0.0      0     0 ?        S    00:26   0:00 [xenwatch]
root        21  0.0  0.0      0     0 ?        S    00:26   0:00 [xenbus]
root        22  0.0  0.0      0     0 ?        S    00:26   0:00 [kworker/0:1]
root        23  0.0  0.0      0     0 ?        S    00:26   0:00 [khungtaskd]
root        24  0.0  0.0      0     0 ?        S<   00:26   0:00 [writeback]
root        25  0.0  0.0      0     0 ?        S<   00:26   0:00 [kintegrityd]
root        26  0.0  0.0      0     0 ?        S<   00:26   0:00 [bioset]
root        27  0.0  0.0      0     0 ?        S<   00:26   0:00 [kblockd]
root        28  0.0  0.0      0     0 ?        S    00:26   0:00 [kworker/1:1]
root        29  0.0  0.0      0     0 ?        S<   00:26   0:00 [md]
root        34  0.0  0.0      0     0 ?        S    00:26   0:00 [kswapd0]
root        35  0.0  0.0      0     0 ?        SN   00:26   0:00 [ksmd]
root        36  0.0  0.0      0     0 ?        SN   00:26   0:00 [khugepaged]
root        37  0.0  0.0      0     0 ?        S    00:26   0:00 [fsnotify_mark]
root        38  0.0  0.0      0     0 ?        S<   00:26   0:00 [crypto]
root        46  0.0  0.0      0     0 ?        S<   00:26   0:00 [kthrotld]
root        47  0.0  0.0      0     0 ?        S    00:26   0:00 [kworker/u30:1]
root        48  0.0  0.0      0     0 ?        S    00:26   0:00 [khvcd]
root        49  0.0  0.0      0     0 ?        S<   00:26   0:00 [kmpath_rdacd]
root        50  0.0  0.0      0     0 ?        S<   00:26   0:00 [kpsmoused]
root        52  0.0  0.0      0     0 ?        S<   00:26   0:00 [ipv6_addrconf]
root        71  0.0  0.0      0     0 ?        S<   00:26   0:00 [deferwq]
root       111  0.0  0.0      0     0 ?        R    00:26   0:00 [kworker/1:2]
root       155  0.0  0.0      0     0 ?        S    00:26   0:00 [kauditd]
root       235  0.0  0.0      0     0 ?        S<   00:26   0:00 [ata_sff]
root       237  0.0  0.0      0     0 ?        S    00:26   0:00 [scsi_eh_0]
root       239  0.0  0.0      0     0 ?        S<   00:26   0:00 [scsi_tmf_0]
root       240  0.0  0.0      0     0 ?        S    00:26   0:00 [scsi_eh_1]
root       242  0.0  0.0      0     0 ?        S<   00:26   0:00 [scsi_tmf_1]
root       250  0.0  0.0      0     0 ?        S<   00:26   0:00 [ttm_swap]
root       277  0.0  0.0      0     0 ?        S    00:26   0:00 [jbd2/xvda2-8]
root       278  0.0  0.0      0     0 ?        S<   00:26   0:00 [ext4-rsv-conver]
root       351  0.0  0.0  36816  3276 ?        Ss   00:26   0:00 /usr/lib/systemd/systemd-journald
root       368  0.0  0.1  46444  4836 ?        Ss   00:26   0:00 /usr/lib/systemd/systemd-udevd
root       450  0.0  0.0      0     0 ?        S    00:26   0:00 [jbd2/xvda1-8]
root       451  0.0  0.0      0     0 ?        S<   00:26   0:00 [ext4-rsv-conver]
root       470  0.0  0.0  55416  1720 ?        S<sl 00:26   0:00 /sbin/auditd -n
root       492  0.0  0.0  19168  1208 ?        Ss   00:26   0:00 /usr/sbin/irqbalance --foreground
root       495  0.0  0.1 219776  3712 ?        Ssl  00:26   0:00 /usr/sbin/rsyslogd -n
polkitd    497  0.0  0.2 527516 10828 ?        Ssl  00:26   0:00 /usr/lib/polkit-1/polkitd --no-debug
avahi      498  0.0  0.0  28112  1512 ?        Ss   00:26   0:00 avahi-daemon: running [spark4.local]
root       502  0.0  0.4 553152 16440 ?        Ssl  00:26   0:00 /usr/bin/python -Es /usr/sbin/tuned -l -P
dbus       503  0.0  0.0  32704  1844 ?        Ssl  00:26   0:00 /bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation
avahi      538  0.0  0.0  27980   232 ?        S    00:26   0:00 avahi-daemon: chroot helper
root       547  0.0  0.0  24192  1676 ?        Ss   00:26   0:00 /usr/lib/systemd/systemd-logind
root       552  0.0  0.0 126220  1668 ?        Ss   00:26   0:00 /usr/sbin/crond -n
root       566  0.0  0.0 110036   848 tty1     Ss+  00:26   0:00 /sbin/agetty --noclear tty1 linux
root       567  0.0  0.0 110036   852 hvc0     Ss+  00:26   0:00 /sbin/agetty --keep-baud 115200 38400 9600 hvc0 vt220
root       572  0.0  0.0      0     0 ?        S<   00:26   0:00 [kworker/0:1H]
root       648  0.0  0.0  82468  1336 ?        Ss   00:26   0:00 /usr/sbin/sshd
root       824  0.0  0.0  88980  2080 ?        Ss   00:26   0:00 /usr/libexec/postfix/master -w
postfix    834  0.0  0.1  89084  3936 ?        S    00:26   0:00 pickup -l -t unix -u
postfix    835  0.0  0.1  89152  3952 ?        S    00:26   0:00 qmgr -l -t unix -u
root      1118  0.0  0.0  11632  1484 ?        S    00:26   0:00 /bin/bash /usr/sbin/xe-daemon -p /var/run/xe-daemon.pid
root      8124  0.0  0.0      0     0 ?        S<   00:26   0:00 [kworker/1:1H]
root      9477  0.0  0.1 142904  5112 ?        Ds   00:28   0:00 sshd: root@pts/0
root      9530  0.0  0.0 115384  2080 pts/0    Ss   00:29   0:00 -bash
root     10936  0.0  0.0 123196   732 ?        Ss   01:01   0:00 /usr/sbin/anacron -s
root     11765  0.0  0.0      0     0 ?        S    01:13   0:00 [kworker/0:0]
root     12427  1.6  5.3 3649332 195128 ?      Sl   01:22   0:03 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64/jre/bin/java -cp /usr/local/spark/conf/:/usr
root     12662  0.0  0.0   4308   348 ?        S    01:25   0:00 sleep 60
root     12663  0.0  0.0 151056  1828 pts/0    R+   01:25   0:00 ps -aux
[root@spark4 sbin]# kill -9 12427
[root@spark4 sbin]# $SPARK_HOME/sbin/start-slaves.sh
spark1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark1.shankar.net.out
spark3: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark3.shankar.net.out
spark2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-spark2.shankar.net.out
[root@spark4 sbin]# $SPARK_HOME/bin/run-example SparkPi
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/02/13 01:26:41 INFO SparkContext: Running Spark version 1.6.2
17/02/13 01:26:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/02/13 01:26:42 INFO SecurityManager: Changing view acls to: root
17/02/13 01:26:42 INFO SecurityManager: Changing modify acls to: root
17/02/13 01:26:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/02/13 01:26:42 INFO Utils: Successfully started service 'sparkDriver' on port 41519.
17/02/13 01:26:43 INFO Slf4jLogger: Slf4jLogger started
17/02/13 01:26:43 INFO Remoting: Starting remoting
17/02/13 01:26:43 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 37629.
17/02/13 01:26:43 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@169.53.128.114:37629]
17/02/13 01:26:43 INFO SparkEnv: Registering MapOutputTracker
17/02/13 01:26:43 INFO SparkEnv: Registering BlockManagerMaster
17/02/13 01:26:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8903d0aa-864e-48a2-ae59-5ee0c296668c
17/02/13 01:26:43 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/02/13 01:26:43 INFO SparkEnv: Registering OutputCommitCoordinator
17/02/13 01:26:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/02/13 01:26:43 INFO SparkUI: Started SparkUI at http://169.53.128.114:4040
17/02/13 01:26:43 INFO HttpFileServer: HTTP File server directory is /tmp/spark-b6632f9c-76f2-4ab2-a886-0c013a100028/httpd-5e0a586b-2d74-47c6-b999-d81e26839798
17/02/13 01:26:43 INFO HttpServer: Starting HTTP Server
17/02/13 01:26:43 INFO Utils: Successfully started service 'HTTP file server' on port 32867.
17/02/13 01:26:44 INFO SparkContext: Added JAR file:/usr/local/spark/lib/spark-examples-1.6.2-hadoop2.6.0.jar at http://169.53.128.114:32867/jars/spark-examples-1.6.2-hadoop2.6.0.jar with timestamp 1486970804067
17/02/13 01:26:44 INFO Executor: Starting executor ID driver on host localhost
17/02/13 01:26:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40382.
17/02/13 01:26:44 INFO NettyBlockTransferService: Server created on 40382
17/02/13 01:26:44 INFO BlockManagerMaster: Trying to register BlockManager
17/02/13 01:26:44 INFO BlockManagerMasterEndpoint: Registering block manager localhost:40382 with 511.1 MB RAM, BlockManagerId(driver, localhost, 40382)
17/02/13 01:26:44 INFO BlockManagerMaster: Registered BlockManager
17/02/13 01:26:44 INFO SparkContext: Starting job: reduce at SparkPi.scala:36
17/02/13 01:26:44 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:36) with 2 output partitions
17/02/13 01:26:44 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:36)
17/02/13 01:26:44 INFO DAGScheduler: Parents of final stage: List()
17/02/13 01:26:44 INFO DAGScheduler: Missing parents: List()
17/02/13 01:26:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:32), which has no missing parents
17/02/13 01:26:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1904.0 B, free 1904.0 B)
17/02/13 01:26:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1218.0 B, free 3.0 KB)
17/02/13 01:26:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:40382 (size: 1218.0 B, free: 511.1 MB)
17/02/13 01:26:44 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/02/13 01:26:44 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:32)
17/02/13 01:26:44 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
17/02/13 01:26:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2157 bytes)
17/02/13 01:26:44 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 2157 bytes)
17/02/13 01:26:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/02/13 01:26:44 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/02/13 01:26:44 INFO Executor: Fetching http://169.53.128.114:32867/jars/spark-examples-1.6.2-hadoop2.6.0.jar with timestamp 1486970804067
17/02/13 01:26:44 INFO Utils: Fetching http://169.53.128.114:32867/jars/spark-examples-1.6.2-hadoop2.6.0.jar to /tmp/spark-b6632f9c-76f2-4ab2-a886-0c013a100028/userFiles-46f120a5-50d5-4c39-b3d7-aa5d8e31d281/fetchFileTemp9067723813028760700.tmp
17/02/13 01:26:45 INFO Executor: Adding file:/tmp/spark-b6632f9c-76f2-4ab2-a886-0c013a100028/userFiles-46f120a5-50d5-4c39-b3d7-aa5d8e31d281/spark-examples-1.6.2-hadoop2.6.0.jar to class loader
17/02/13 01:26:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1031 bytes result sent to driver
17/02/13 01:26:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 574 ms on localhost (1/2)
17/02/13 01:26:45 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1031 bytes result sent to driver
17/02/13 01:26:45 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 573 ms on localhost (2/2)
17/02/13 01:26:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/02/13 01:26:45 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:36) finished in 0.610 s
17/02/13 01:26:45 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:36, took 0.831218 s
Pi is roughly 3.14508
17/02/13 01:26:45 INFO SparkUI: Stopped Spark web UI at http://169.53.128.114:4040
17/02/13 01:26:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/02/13 01:26:45 INFO MemoryStore: MemoryStore cleared
17/02/13 01:26:45 INFO BlockManager: BlockManager stopped
17/02/13 01:26:45 INFO BlockManagerMaster: BlockManagerMaster stopped
17/02/13 01:26:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/02/13 01:26:45 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/02/13 01:26:45 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/02/13 01:26:45 INFO SparkContext: Successfully stopped SparkContext
17/02/13 01:26:45 INFO ShutdownHookManager: Shutdown hook called
17/02/13 01:26:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-b6632f9c-76f2-4ab2-a886-0c013a100028/httpd-5e0a586b-2d74-47c6-b999-d81e26839798
17/02/13 01:26:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-b6632f9c-76f2-4ab2-a886-0c013a100028
[root@spark4 sbin]# $SPARK_HOME/bin/spark-submit $SPARK_HOME/examples/src/main/python/pi.py
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/02/13 01:28:02 INFO SparkContext: Running Spark version 1.6.2
17/02/13 01:28:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/02/13 01:28:02 INFO SecurityManager: Changing view acls to: root
17/02/13 01:28:02 INFO SecurityManager: Changing modify acls to: root
17/02/13 01:28:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/02/13 01:28:02 INFO Utils: Successfully started service 'sparkDriver' on port 37468.
17/02/13 01:28:03 INFO Slf4jLogger: Slf4jLogger started
17/02/13 01:28:03 INFO Remoting: Starting remoting
17/02/13 01:28:03 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 46708.
17/02/13 01:28:03 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@169.53.128.114:46708]
17/02/13 01:28:03 INFO SparkEnv: Registering MapOutputTracker
17/02/13 01:28:03 INFO SparkEnv: Registering BlockManagerMaster
17/02/13 01:28:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-39820f63-dc15-4b11-bfe7-3979e260ab34
17/02/13 01:28:03 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/02/13 01:28:03 INFO SparkEnv: Registering OutputCommitCoordinator
17/02/13 01:28:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/02/13 01:28:03 INFO SparkUI: Started SparkUI at http://169.53.128.114:4040
17/02/13 01:28:03 INFO Utils: Copying /usr/local/spark/examples/src/main/python/pi.py to /tmp/spark-b2866f58-a3db-4536-883e-189837f49fa0/userFiles-12c3ec30-f6f2-4539-ae50-8744ee344470/pi.py
17/02/13 01:28:03 INFO SparkContext: Added file file:/usr/local/spark/examples/src/main/python/pi.py at file:/usr/local/spark/examples/src/main/python/pi.py with timestamp 1486970883778
17/02/13 01:28:03 INFO Executor: Starting executor ID driver on host localhost
17/02/13 01:28:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35188.
17/02/13 01:28:03 INFO NettyBlockTransferService: Server created on 35188
17/02/13 01:28:03 INFO BlockManagerMaster: Trying to register BlockManager
17/02/13 01:28:03 INFO BlockManagerMasterEndpoint: Registering block manager localhost:35188 with 511.1 MB RAM, BlockManagerId(driver, localhost, 35188)
17/02/13 01:28:03 INFO BlockManagerMaster: Registered BlockManager
17/02/13 01:28:04 INFO SparkContext: Starting job: reduce at /usr/local/spark/examples/src/main/python/pi.py:39
17/02/13 01:28:04 INFO DAGScheduler: Got job 0 (reduce at /usr/local/spark/examples/src/main/python/pi.py:39) with 2 output partitions
17/02/13 01:28:04 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /usr/local/spark/examples/src/main/python/pi.py:39)
17/02/13 01:28:04 INFO DAGScheduler: Parents of final stage: List()
17/02/13 01:28:04 INFO DAGScheduler: Missing parents: List()
17/02/13 01:28:04 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /usr/local/spark/examples/src/main/python/pi.py:39), which has no missing parents
17/02/13 01:28:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KB, free 4.0 KB)
17/02/13 01:28:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.7 KB, free 6.7 KB)
17/02/13 01:28:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:35188 (size: 2.7 KB, free: 511.1 MB)
17/02/13 01:28:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/02/13 01:28:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /usr/local/spark/examples/src/main/python/pi.py:39)
17/02/13 01:28:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
17/02/13 01:28:04 WARN TaskSetManager: Stage 0 contains a task of very large size (365 KB). The maximum recommended task size is 100 KB.
17/02/13 01:28:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 374521 bytes)
17/02/13 01:28:04 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 502324 bytes)
17/02/13 01:28:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/02/13 01:28:04 INFO Executor: Fetching file:/usr/local/spark/examples/src/main/python/pi.py with timestamp 1486970883778
17/02/13 01:28:04 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/02/13 01:28:04 INFO Utils: /usr/local/spark/examples/src/main/python/pi.py has been previously copied to /tmp/spark-b2866f58-a3db-4536-883e-189837f49fa0/userFiles-12c3ec30-f6f2-4539-ae50-8744ee344470/pi.py
17/02/13 01:28:05 INFO PythonRunner: Times: total = 300, boot = 176, init = 6, finish = 118
17/02/13 01:28:05 INFO PythonRunner: Times: total = 308, boot = 177, init = 8, finish = 123
17/02/13 01:28:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 998 bytes result sent to driver
17/02/13 01:28:05 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 998 bytes result sent to driver
17/02/13 01:28:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 433 ms on localhost (1/2)
17/02/13 01:28:05 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 414 ms on localhost (2/2)
17/02/13 01:28:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/02/13 01:28:05 INFO DAGScheduler: ResultStage 0 (reduce at /usr/local/spark/examples/src/main/python/pi.py:39) finished in 0.453 s
17/02/13 01:28:05 INFO DAGScheduler: Job 0 finished: reduce at /usr/local/spark/examples/src/main/python/pi.py:39, took 0.661423 s
Pi is roughly 3.139260
17/02/13 01:28:05 INFO SparkUI: Stopped Spark web UI at http://169.53.128.114:4040
17/02/13 01:28:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/02/13 01:28:05 INFO MemoryStore: MemoryStore cleared
17/02/13 01:28:05 INFO BlockManager: BlockManager stopped
17/02/13 01:28:05 INFO BlockManagerMaster: BlockManagerMaster stopped
17/02/13 01:28:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/02/13 01:28:05 INFO SparkContext: Successfully stopped SparkContext
17/02/13 01:28:05 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/02/13 01:28:05 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/02/13 01:28:05 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/02/13 01:28:06 INFO ShutdownHookManager: Shutdown hook called
17/02/13 01:28:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-b2866f58-a3db-4536-883e-189837f49fa0
17/02/13 01:28:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-b2866f58-a3db-4536-883e-189837f49fa0/pyspark-71e442be-72c8-4329-8ff9-41393ad32bac
[root@spark4 sbin]# $SPARK_HOME/bin/spark-shell
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel("INFO")
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.2
      /_/

Using Scala version 2.10.5 (OpenJDK 64-Bit Server VM, Java 1.8.0_121)
Type in expressions to have them evaluated.
Type :help for more information.
Spark context available as sc.
17/02/13 01:28:39 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/02/13 01:28:39 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/02/13 01:28:43 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/02/13 01:28:43 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/02/13 01:28:44 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/02/13 01:28:45 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/02/13 01:28:48 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/02/13 01:28:48 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
SQL context available as sqlContext.

scala> val textFile = sc.textFile("README.md")
textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:27

scala> textFile.count()
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/usr/local/spark/sbin/README.md
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1157)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:35)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:37)
	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:39)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:41)
	at $iwC$$iwC$$iwC.<init>(<console>:43)
	at $iwC$$iwC.<init>(<console>:45)
	at $iwC.<init>(<console>:47)
	at <init>(<console>:49)
	at .<init>(<console>:53)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)


scala> textFile.first()
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/usr/local/spark/sbin/README.md
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)
	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1307)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1302)
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1342)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1341)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:35)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:37)
	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:39)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:41)
	at $iwC$$iwC$$iwC.<init>(<console>:43)
	at $iwC$$iwC.<init>(<console>:45)
	at $iwC.<init>(<console>:47)
	at <init>(<console>:49)
	at .<init>(<console>:53)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)


scala> 

scala> 

scala> 

scala> exit
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
[root@spark4 sbin]# cd $SPARK_HOME
[root@spark4 spark]# pwd
/usr/local/spark
[root@spark4 spark]# $SPARK_HOME/bin/spark-shell
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel("INFO")
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.2
      /_/

Using Scala version 2.10.5 (OpenJDK 64-Bit Server VM, Java 1.8.0_121)
Type in expressions to have them evaluated.
Type :help for more information.
Spark context available as sc.
17/02/13 01:31:58 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/02/13 01:31:58 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/02/13 01:32:02 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/02/13 01:32:02 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/02/13 01:32:03 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/02/13 01:32:03 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/02/13 01:32:06 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/02/13 01:32:07 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
SQL context available as sqlContext.

scala> val textFile = sc.textFile("README.md")
textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:27

scala> textFile.count()
res0: Long = 95

scala> textFile.first()
res1: String = # Apache Spark

scala> val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:29

scala> linesWithSpark.count()
res2: Long = 17

scala> Stopping spark context.
[root@spark4 spark]# 
[root@spark4 spark]# 
[root@spark4 spark]# 
[root@spark4 spark]# sbt
Getting org.scala-sbt sbt 0.13.13 ...
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt/0.13.13/jars/sbt.jar ...
	[SUCCESSFUL ] org.scala-sbt#sbt;0.13.13!sbt.jar (1503ms)
downloading https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.6/scala-library-2.10.6.jar ...
	[SUCCESSFUL ] org.scala-lang#scala-library;2.10.6!scala-library.jar (306ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/main/0.13.13/jars/main.jar ...
	[SUCCESSFUL ] org.scala-sbt#main;0.13.13!main.jar (7046ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/compiler-interface/0.13.13/jars/compiler-interface.jar ...
	[SUCCESSFUL ] org.scala-sbt#compiler-interface;0.13.13!compiler-interface.jar (1577ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/actions/0.13.13/jars/actions.jar ...
	[SUCCESSFUL ] org.scala-sbt#actions;0.13.13!actions.jar (1551ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/main-settings/0.13.13/jars/main-settings.jar ...
	[SUCCESSFUL ] org.scala-sbt#main-settings;0.13.13!main-settings.jar (2225ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/interface/0.13.13/jars/interface.jar ...
	[SUCCESSFUL ] org.scala-sbt#interface;0.13.13!interface.jar (1566ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/io/0.13.13/jars/io.jar ...
	[SUCCESSFUL ] org.scala-sbt#io;0.13.13!io.jar (1897ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/ivy/0.13.13/jars/ivy.jar ...
	[SUCCESSFUL ] org.scala-sbt#ivy;0.13.13!ivy.jar (1641ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/logging/0.13.13/jars/logging.jar ...
	[SUCCESSFUL ] org.scala-sbt#logging;0.13.13!logging.jar (1554ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/logic/0.13.13/jars/logic.jar ...
	[SUCCESSFUL ] org.scala-sbt#logic;0.13.13!logic.jar (1614ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/process/0.13.13/jars/process.jar ...
	[SUCCESSFUL ] org.scala-sbt#process;0.13.13!process.jar (1688ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/run/0.13.13/jars/run.jar ...
	[SUCCESSFUL ] org.scala-sbt#run;0.13.13!run.jar (1545ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/command/0.13.13/jars/command.jar ...
	[SUCCESSFUL ] org.scala-sbt#command;0.13.13!command.jar (1549ms)
downloading https://repo1.maven.org/maven2/org/scala-sbt/launcher-interface/1.0.0-M1/launcher-interface-1.0.0-M1.jar ...
	[SUCCESSFUL ] org.scala-sbt#launcher-interface;1.0.0-M1!launcher-interface.jar (20ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/classpath/0.13.13/jars/classpath.jar ...
	[SUCCESSFUL ] org.scala-sbt#classpath;0.13.13!classpath.jar (1545ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/completion/0.13.13/jars/completion.jar ...
	[SUCCESSFUL ] org.scala-sbt#completion;0.13.13!completion.jar (1674ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/api/0.13.13/jars/api.jar ...
	[SUCCESSFUL ] org.scala-sbt#api;0.13.13!api.jar (1660ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/compiler-integration/0.13.13/jars/compiler-integration.jar ...
	[SUCCESSFUL ] org.scala-sbt#compiler-integration;0.13.13!compiler-integration.jar (1591ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/compiler-ivy-integration/0.13.13/jars/compiler-ivy-integration.jar ...
	[SUCCESSFUL ] org.scala-sbt#compiler-ivy-integration;0.13.13!compiler-ivy-integration.jar (1556ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/relation/0.13.13/jars/relation.jar ...
	[SUCCESSFUL ] org.scala-sbt#relation;0.13.13!relation.jar (1528ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/task-system/0.13.13/jars/task-system.jar ...
	[SUCCESSFUL ] org.scala-sbt#task-system;0.13.13!task-system.jar (1769ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/tasks/0.13.13/jars/tasks.jar ...
	[SUCCESSFUL ] org.scala-sbt#tasks;0.13.13!tasks.jar (1596ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/tracking/0.13.13/jars/tracking.jar ...
	[SUCCESSFUL ] org.scala-sbt#tracking;0.13.13!tracking.jar (1557ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/testing/0.13.13/jars/testing.jar ...
	[SUCCESSFUL ] org.scala-sbt#testing;0.13.13!testing.jar (1591ms)
downloading https://repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.10.6/scala-compiler-2.10.6.jar ...
	[SUCCESSFUL ] org.scala-lang#scala-compiler;2.10.6!scala-compiler.jar (551ms)
downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.10.6/scala-reflect-2.10.6.jar ...
	[SUCCESSFUL ] org.scala-lang#scala-reflect;2.10.6!scala-reflect.jar (127ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/control/0.13.13/jars/control.jar ...
	[SUCCESSFUL ] org.scala-sbt#control;0.13.13!control.jar (1542ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/collections/0.13.13/jars/collections.jar ...
	[SUCCESSFUL ] org.scala-sbt#collections;0.13.13!collections.jar (1630ms)
downloading https://repo1.maven.org/maven2/jline/jline/2.13/jline-2.13.jar ...
	[SUCCESSFUL ] jline#jline;2.13!jline.jar (16ms)
downloading https://repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.11/jansi-1.11.jar ...
	[SUCCESSFUL ] org.fusesource.jansi#jansi;1.11!jansi.jar (11ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/classfile/0.13.13/jars/classfile.jar ...
	[SUCCESSFUL ] org.scala-sbt#classfile;0.13.13!classfile.jar (1592ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/incremental-compiler/0.13.13/jars/incremental-compiler.jar ...
	[SUCCESSFUL ] org.scala-sbt#incremental-compiler;0.13.13!incremental-compiler.jar (1630ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/compile/0.13.13/jars/compile.jar ...
	[SUCCESSFUL ] org.scala-sbt#compile;0.13.13!compile.jar (1616ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/persist/0.13.13/jars/persist.jar ...
	[SUCCESSFUL ] org.scala-sbt#persist;0.13.13!persist.jar (1599ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-tools.sbinary/sbinary_2.10/0.4.2/jars/sbinary_2.10.jar ...
	[SUCCESSFUL ] org.scala-tools.sbinary#sbinary_2.10;0.4.2!sbinary_2.10.jar (1566ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/cross/0.13.13/jars/cross.jar ...
	[SUCCESSFUL ] org.scala-sbt#cross;0.13.13!cross.jar (1549ms)
downloading https://repo1.maven.org/maven2/org/scala-sbt/ivy/ivy/2.3.0-sbt-2cf13e211b2cb31f0d3b317289dca70eca3362f6/ivy-2.3.0-sbt-2cf13e211b2cb31f0d3b317289dca70eca3362f6.jar ...
	[SUCCESSFUL ] org.scala-sbt.ivy#ivy;2.3.0-sbt-2cf13e211b2cb31f0d3b317289dca70eca3362f6!ivy.jar (54ms)
downloading https://repo1.maven.org/maven2/com/jcraft/jsch/0.1.50/jsch-0.1.50.jar ...
	[SUCCESSFUL ] com.jcraft#jsch;0.1.50!jsch.jar (18ms)
downloading https://repo1.maven.org/maven2/org/scala-sbt/serialization_2.10/0.1.2/serialization_2.10-0.1.2.jar ...
	[SUCCESSFUL ] org.scala-sbt#serialization_2.10;0.1.2!serialization_2.10.jar (18ms)
downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-pickling_2.10/0.10.1/scala-pickling_2.10-0.10.1.jar ...
	[SUCCESSFUL ] org.scala-lang.modules#scala-pickling_2.10;0.10.1!scala-pickling_2.10.jar (56ms)
downloading https://repo1.maven.org/maven2/org/json4s/json4s-core_2.10/3.2.10/json4s-core_2.10-3.2.10.jar ...
	[SUCCESSFUL ] org.json4s#json4s-core_2.10;3.2.10!json4s-core_2.10.jar (28ms)
downloading https://repo1.maven.org/maven2/org/spire-math/jawn-parser_2.10/0.6.0/jawn-parser_2.10-0.6.0.jar ...
	[SUCCESSFUL ] org.spire-math#jawn-parser_2.10;0.6.0!jawn-parser_2.10.jar (9ms)
downloading https://repo1.maven.org/maven2/org/spire-math/json4s-support_2.10/0.6.0/json4s-support_2.10-0.6.0.jar ...
	[SUCCESSFUL ] org.spire-math#json4s-support_2.10;0.6.0!json4s-support_2.10.jar (7ms)
downloading https://repo1.maven.org/maven2/org/scalamacros/quasiquotes_2.10/2.0.1/quasiquotes_2.10-2.0.1.jar ...
	[SUCCESSFUL ] org.scalamacros#quasiquotes_2.10;2.0.1!quasiquotes_2.10.jar (34ms)
downloading https://repo1.maven.org/maven2/org/json4s/json4s-ast_2.10/3.2.10/json4s-ast_2.10-3.2.10.jar ...
	[SUCCESSFUL ] org.json4s#json4s-ast_2.10;3.2.10!json4s-ast_2.10.jar (9ms)
downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar ...
	[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.6!paranamer.jar (10ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/cache/0.13.13/jars/cache.jar ...
	[SUCCESSFUL ] org.scala-sbt#cache;0.13.13!cache.jar (1609ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/test-agent/0.13.13/jars/test-agent.jar ...
	[SUCCESSFUL ] org.scala-sbt#test-agent;0.13.13!test-agent.jar (1510ms)
downloading https://repo1.maven.org/maven2/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar ...
	[SUCCESSFUL ] org.scala-sbt#test-interface;1.0!test-interface.jar (7ms)
downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/apply-macro/0.13.13/jars/apply-macro.jar ...
	[SUCCESSFUL ] org.scala-sbt#apply-macro;0.13.13!apply-macro.jar (1588ms)
downloading https://repo1.maven.org/maven2/org/scala-sbt/template-resolver/0.1/template-resolver-0.1.jar ...
	[SUCCESSFUL ] org.scala-sbt#template-resolver;0.1!template-resolver.jar (8ms)
:: retrieving :: org.scala-sbt#boot-app
	confs: [default]
	50 artifacts copied, 0 already retrieved (17645kB/64ms)
Getting Scala 2.10.6 (for sbt)...
downloading https://repo1.maven.org/maven2/org/scala-lang/jline/2.10.6/jline-2.10.6.jar ...
	[SUCCESSFUL ] org.scala-lang#jline;2.10.6!jline.jar (16ms)
downloading https://repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.4/jansi-1.4.jar ...
	[SUCCESSFUL ] org.fusesource.jansi#jansi;1.4!jansi.jar (10ms)
:: retrieving :: org.scala-sbt#boot-scala
	confs: [default]
	5 artifacts copied, 0 already retrieved (24494kB/50ms)
[info] Set current project to spark (in build file:/usr/local/spark/)
> [root@spark4 spark]# vi SimpleApp.scala
[root@spark4 spark]# vi build.sbt
[root@spark4 spark]# sbt package
[info] Set current project to Simple Project (in build file:/usr/local/spark/)
[info] Updating {file:/usr/local/spark/}spark...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[info] downloading https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar ...
[info] 	[SUCCESSFUL ] org.scala-lang#scala-library;2.10.4!scala-library.jar (297ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.10/1.6.1/spark-core_2.10-1.6.1.jar ...
[info] 	[SUCCESSFUL ] org.apache.spark#spark-core_2.10;1.6.1!spark-core_2.10.jar (452ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar ...
[info] 	[SUCCESSFUL ] org.apache.avro#avro-mapred;1.7.7!avro-mapred.jar (17ms)
[info] downloading https://repo1.maven.org/maven2/com/twitter/chill_2.10/0.5.0/chill_2.10-0.5.0.jar ...
[info] 	[SUCCESSFUL ] com.twitter#chill_2.10;0.5.0!chill_2.10.jar (16ms)
[info] downloading https://repo1.maven.org/maven2/com/twitter/chill-java/0.5.0/chill-java-0.5.0.jar ...
[info] 	[SUCCESSFUL ] com.twitter#chill-java;0.5.0!chill-java.jar (10ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar ...
[info] 	[SUCCESSFUL ] org.apache.xbean#xbean-asm5-shaded;4.4!xbean-asm5-shaded.jar(bundle) (13ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/2.2.0/hadoop-client-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-client;2.2.0!hadoop-client.jar (7ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/spark/spark-launcher_2.10/1.6.1/spark-launcher_2.10-1.6.1.jar ...
[info] 	[SUCCESSFUL ] org.apache.spark#spark-launcher_2.10;1.6.1!spark-launcher_2.10.jar (15ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/spark/spark-network-common_2.10/1.6.1/spark-network-common_2.10-1.6.1.jar ...
[info] 	[SUCCESSFUL ] org.apache.spark#spark-network-common_2.10;1.6.1!spark-network-common_2.10.jar (99ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/spark/spark-network-shuffle_2.10/1.6.1/spark-network-shuffle_2.10-1.6.1.jar ...
[info] 	[SUCCESSFUL ] org.apache.spark#spark-network-shuffle_2.10;1.6.1!spark-network-shuffle_2.10.jar (11ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.10/1.6.1/spark-unsafe_2.10-1.6.1.jar ...
[info] 	[SUCCESSFUL ] org.apache.spark#spark-unsafe_2.10;1.6.1!spark-unsafe_2.10.jar (8ms)
[info] downloading https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar ...
[info] 	[SUCCESSFUL ] net.java.dev.jets3t#jets3t;0.7.1!jets3t.jar (20ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.curator#curator-recipes;2.4.0!curator-recipes.jar(bundle) (16ms)
[info] downloading https://repo1.maven.org/maven2/org/eclipse/jetty/orbit/javax.servlet/3.0.0.v201112011016/javax.servlet-3.0.0.v201112011016.jar ...
[info] 	[SUCCESSFUL ] org.eclipse.jetty.orbit#javax.servlet;3.0.0.v201112011016!javax.servlet.jar(orbit) (15ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar ...
[info] 	[SUCCESSFUL ] org.apache.commons#commons-lang3;3.3.2!commons-lang3.jar (21ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar ...
[info] 	[SUCCESSFUL ] org.apache.commons#commons-math3;3.4.1!commons-math3.jar (81ms)
[info] downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar ...
[info] 	[SUCCESSFUL ] com.google.code.findbugs#jsr305;1.3.9!jsr305.jar (10ms)
[info] downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar ...
[info] 	[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.10!slf4j-api.jar (8ms)
[info] downloading https://repo1.maven.org/maven2/org/slf4j/jul-to-slf4j/1.7.10/jul-to-slf4j-1.7.10.jar ...
[info] 	[SUCCESSFUL ] org.slf4j#jul-to-slf4j;1.7.10!jul-to-slf4j.jar (17ms)
[info] downloading https://repo1.maven.org/maven2/org/slf4j/jcl-over-slf4j/1.7.10/jcl-over-slf4j-1.7.10.jar ...
[info] 	[SUCCESSFUL ] org.slf4j#jcl-over-slf4j;1.7.10!jcl-over-slf4j.jar (14ms)
[info] downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...
[info] 	[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (24ms)
[info] downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar ...
[info] 	[SUCCESSFUL ] org.slf4j#slf4j-log4j12;1.7.10!slf4j-log4j12.jar (6ms)
[info] downloading https://repo1.maven.org/maven2/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar ...
[info] 	[SUCCESSFUL ] com.ning#compress-lzf;1.0.3!compress-lzf.jar(bundle) (11ms)
[info] downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.2/snappy-java-1.1.2.jar ...
[info] 	[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.2!snappy-java.jar(bundle) (34ms)
[info] downloading https://repo1.maven.org/maven2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar ...
[info] 	[SUCCESSFUL ] net.jpountz.lz4#lz4;1.3.0!lz4.jar (19ms)
[info] downloading https://repo1.maven.org/maven2/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar ...
[info] 	[SUCCESSFUL ] org.roaringbitmap#RoaringBitmap;0.5.11!RoaringBitmap.jar(bundle) (16ms)
[info] downloading https://repo1.maven.org/maven2/commons-net/commons-net/2.2/commons-net-2.2.jar ...
[info] 	[SUCCESSFUL ] commons-net#commons-net;2.2!commons-net.jar (14ms)
[info] downloading https://repo1.maven.org/maven2/com/typesafe/akka/akka-remote_2.10/2.3.11/akka-remote_2.10-2.3.11.jar ...
[info] 	[SUCCESSFUL ] com.typesafe.akka#akka-remote_2.10;2.3.11!akka-remote_2.10.jar (57ms)
[info] downloading https://repo1.maven.org/maven2/com/typesafe/akka/akka-slf4j_2.10/2.3.11/akka-slf4j_2.10-2.3.11.jar ...
[info] 	[SUCCESSFUL ] com.typesafe.akka#akka-slf4j_2.10;2.3.11!akka-slf4j_2.10.jar (8ms)
[info] downloading https://repo1.maven.org/maven2/org/json4s/json4s-jackson_2.10/3.2.10/json4s-jackson_2.10-3.2.10.jar ...
[info] 	[SUCCESSFUL ] org.json4s#json4s-jackson_2.10;3.2.10!json4s-jackson_2.10.jar (9ms)
[info] downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar ...
[info] 	[SUCCESSFUL ] com.sun.jersey#jersey-server;1.9!jersey-server.jar(bundle) (32ms)
[info] downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar ...
[info] 	[SUCCESSFUL ] com.sun.jersey#jersey-core;1.9!jersey-core.jar(bundle) (23ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/mesos/mesos/0.21.1/mesos-0.21.1-shaded-protobuf.jar ...
[info] 	[SUCCESSFUL ] org.apache.mesos#mesos;0.21.1!mesos.jar (56ms)
[info] downloading https://repo1.maven.org/maven2/io/netty/netty-all/4.0.29.Final/netty-all-4.0.29.Final.jar ...
[info] 	[SUCCESSFUL ] io.netty#netty-all;4.0.29.Final!netty-all.jar (85ms)
[info] downloading https://repo1.maven.org/maven2/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar ...
[info] 	[SUCCESSFUL ] com.clearspring.analytics#stream;2.7.0!stream.jar (15ms)
[info] downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar ...
[info] 	[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;3.1.2!metrics-core.jar(bundle) (11ms)
[info] downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar ...
[info] 	[SUCCESSFUL ] io.dropwizard.metrics#metrics-jvm;3.1.2!metrics-jvm.jar(bundle) (8ms)
[info] downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar ...
[info] 	[SUCCESSFUL ] io.dropwizard.metrics#metrics-json;3.1.2!metrics-json.jar(bundle) (8ms)
[info] downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar ...
[info] 	[SUCCESSFUL ] io.dropwizard.metrics#metrics-graphite;3.1.2!metrics-graphite.jar(bundle) (9ms)
[info] downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.4.4/jackson-databind-2.4.4.jar ...
[info] 	[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.4.4!jackson-databind.jar(bundle) (48ms)
[info] downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.10/2.4.4/jackson-module-scala_2.10-2.4.4.jar ...
[info] 	[SUCCESSFUL ] com.fasterxml.jackson.module#jackson-module-scala_2.10;2.4.4!jackson-module-scala_2.10.jar(bundle) (28ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.ivy#ivy;2.4.0!ivy.jar (66ms)
[info] downloading https://repo1.maven.org/maven2/oro/oro/2.0.8/oro-2.0.8.jar ...
[info] 	[SUCCESSFUL ] oro#oro;2.0.8!oro.jar (11ms)
[info] downloading https://repo1.maven.org/maven2/org/tachyonproject/tachyon-client/0.8.2/tachyon-client-0.8.2.jar ...
[info] 	[SUCCESSFUL ] org.tachyonproject#tachyon-client;0.8.2!tachyon-client.jar (96ms)
[info] downloading https://repo1.maven.org/maven2/net/razorvine/pyrolite/4.9/pyrolite-4.9.jar ...
[info] 	[SUCCESSFUL ] net.razorvine#pyrolite;4.9!pyrolite.jar (10ms)
[info] downloading https://repo1.maven.org/maven2/net/sf/py4j/py4j/0.9/py4j-0.9.jar ...
[info] 	[SUCCESSFUL ] net.sf.py4j#py4j;0.9!py4j.jar (10ms)
[info] downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
[info] 	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (7ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar ...
[info] 	[SUCCESSFUL ] org.apache.avro#avro-ipc;1.7.7!avro-ipc.jar (23ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar ...
[info] 	[SUCCESSFUL ] org.apache.avro#avro-ipc;1.7.7!avro-ipc.jar (14ms)
[info] downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...
[info] 	[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (15ms)
[info] downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...
[info] 	[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (35ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar ...
[info] 	[SUCCESSFUL ] org.apache.avro#avro;1.7.7!avro.jar(bundle) (23ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...
[info] 	[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (17ms)
[info] downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...
[info] 	[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (12ms)
[info] downloading https://repo1.maven.org/maven2/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar ...
[info] 	[SUCCESSFUL ] com.esotericsoftware.kryo#kryo;2.21!kryo.jar(bundle) (25ms)
[info] downloading https://repo1.maven.org/maven2/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar ...
[info] 	[SUCCESSFUL ] com.esotericsoftware.reflectasm#reflectasm;1.07!reflectasm.jar (13ms)
[info] downloading https://repo1.maven.org/maven2/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar ...
[info] 	[SUCCESSFUL ] com.esotericsoftware.minlog#minlog;1.2!minlog.jar (7ms)
[info] downloading https://repo1.maven.org/maven2/org/objenesis/objenesis/1.2/objenesis-1.2.jar ...
[info] 	[SUCCESSFUL ] org.objenesis#objenesis;1.2!objenesis.jar (11ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.2.0/hadoop-common-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-common;2.2.0!hadoop-common.jar (117ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs/2.2.0/hadoop-hdfs-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-hdfs;2.2.0!hadoop-hdfs.jar (207ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-app/2.2.0/hadoop-mapreduce-client-app-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-app;2.2.0!hadoop-mapreduce-client-app.jar (26ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-api/2.2.0/hadoop-yarn-api-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-api;2.2.0!hadoop-yarn-api.jar (54ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.2.0/hadoop-mapreduce-client-core-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-core;2.2.0!hadoop-mapreduce-client-core.jar (62ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.2.0/hadoop-mapreduce-client-jobclient-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-jobclient;2.2.0!hadoop-mapreduce-client-jobclient.jar (10ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/2.2.0/hadoop-annotations-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;2.2.0!hadoop-annotations.jar (7ms)
[info] downloading https://repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar ...
[info] 	[SUCCESSFUL ] commons-cli#commons-cli;1.2!commons-cli.jar (8ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math/2.1/commons-math-2.1.jar ...
[info] 	[SUCCESSFUL ] org.apache.commons#commons-math;2.1!commons-math.jar (41ms)
[info] downloading https://repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar ...
[info] 	[SUCCESSFUL ] xmlenc#xmlenc;0.52!xmlenc.jar (7ms)
[info] downloading https://repo1.maven.org/maven2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar ...
[info] 	[SUCCESSFUL ] commons-httpclient#commons-httpclient;3.1!commons-httpclient.jar (19ms)
[info] downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar ...
[info] 	[SUCCESSFUL ] commons-codec#commons-codec;1.4!commons-codec.jar (10ms)
[info] downloading https://repo1.maven.org/maven2/commons-lang/commons-lang/2.5/commons-lang-2.5.jar ...
[info] 	[SUCCESSFUL ] commons-lang#commons-lang;2.5!commons-lang.jar (16ms)
[info] downloading https://repo1.maven.org/maven2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar ...
[info] 	[SUCCESSFUL ] commons-configuration#commons-configuration;1.6!commons-configuration.jar (18ms)
[info] downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar ...
[info] 	[SUCCESSFUL ] com.google.protobuf#protobuf-java;2.5.0!protobuf-java.jar(bundle) (30ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/2.2.0/hadoop-auth-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-auth;2.2.0!hadoop-auth.jar (10ms)
[info] downloading https://repo1.maven.org/maven2/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar ...
[info] 	[SUCCESSFUL ] commons-collections#commons-collections;3.2.1!commons-collections.jar (29ms)
[info] downloading https://repo1.maven.org/maven2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar ...
[info] 	[SUCCESSFUL ] commons-digester#commons-digester;1.8!commons-digester.jar (13ms)
[info] downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar ...
[info] 	[SUCCESSFUL ] commons-beanutils#commons-beanutils-core;1.8.0!commons-beanutils-core.jar (14ms)
[info] downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar ...
[info] 	[SUCCESSFUL ] commons-beanutils#commons-beanutils;1.7.0!commons-beanutils.jar (15ms)
[info] downloading https://repo1.maven.org/maven2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar ...
[info] 	[SUCCESSFUL ] org.mortbay.jetty#jetty-util;6.1.26!jetty-util.jar (14ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/2.2.0/hadoop-mapreduce-client-common-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-common;2.2.0!hadoop-mapreduce-client-common.jar (34ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.2.0/hadoop-mapreduce-client-shuffle-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-shuffle;2.2.0!hadoop-mapreduce-client-shuffle.jar (10ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-common/2.2.0/hadoop-yarn-common-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-common;2.2.0!hadoop-yarn-common.jar (80ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-client/2.2.0/hadoop-yarn-client-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-client;2.2.0!hadoop-yarn-client.jar (12ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-common/2.2.0/hadoop-yarn-server-common-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-server-common;2.2.0!hadoop-yarn-server-common.jar (13ms)
[info] downloading https://repo1.maven.org/maven2/com/google/inject/guice/3.0/guice-3.0.jar ...
[info] 	[SUCCESSFUL ] com.google.inject#guice;3.0!guice.jar (32ms)
[info] downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-test-framework/jersey-test-framework-grizzly2/1.9/jersey-test-framework-grizzly2-1.9.jar ...
[info] 	[SUCCESSFUL ] com.sun.jersey.jersey-test-framework#jersey-test-framework-grizzly2;1.9!jersey-test-framework-grizzly2.jar (7ms)
[info] downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar ...
[info] 	[SUCCESSFUL ] com.sun.jersey#jersey-json;1.9!jersey-json.jar(bundle) (12ms)
[info] downloading https://repo1.maven.org/maven2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar ...
[info] 	[SUCCESSFUL ] com.sun.jersey.contribs#jersey-guice;1.9!jersey-guice.jar (7ms)
[info] downloading https://repo1.maven.org/maven2/javax/inject/javax.inject/1/javax.inject-1.jar ...
[info] 	[SUCCESSFUL ] javax.inject#javax.inject;1!javax.inject.jar (6ms)
[info] downloading https://repo1.maven.org/maven2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar ...
[info] 	[SUCCESSFUL ] aopalliance#aopalliance;1.0!aopalliance.jar (6ms)
[info] downloading https://repo1.maven.org/maven2/org/sonatype/sisu/inject/cglib/2.2.1-v20090111/cglib-2.2.1-v20090111.jar ...
[info] 	[SUCCESSFUL ] org.sonatype.sisu.inject#cglib;2.2.1-v20090111!cglib.jar (16ms)
[info] downloading https://repo1.maven.org/maven2/asm/asm/3.1/asm-3.1.jar ...
[info] 	[SUCCESSFUL ] asm#asm;3.1!asm.jar (9ms)
[info] downloading https://repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar ...
[info] 	[SUCCESSFUL ] org.codehaus.jettison#jettison;1.1!jettison.jar(bundle) (8ms)
[info] downloading https://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar ...
[info] 	[SUCCESSFUL ] com.sun.xml.bind#jaxb-impl;2.2.3-1!jaxb-impl.jar (38ms)
[info] downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar ...
[info] 	[SUCCESSFUL ] org.codehaus.jackson#jackson-jaxrs;1.8.3!jackson-jaxrs.jar (7ms)
[info] downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar ...
[info] 	[SUCCESSFUL ] org.codehaus.jackson#jackson-xc;1.8.3!jackson-xc.jar (9ms)
[info] downloading https://repo1.maven.org/maven2/stax/stax-api/1.0.1/stax-api-1.0.1.jar ...
[info] 	[SUCCESSFUL ] stax#stax-api;1.0.1!stax-api.jar (7ms)
[info] downloading https://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar ...
[info] 	[SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.2!jaxb-api.jar (10ms)
[info] downloading https://repo1.maven.org/maven2/javax/activation/activation/1.1/activation-1.1.jar ...
[info] 	[SUCCESSFUL ] javax.activation#activation;1.1!activation.jar (8ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.2.0/hadoop-yarn-server-nodemanager-2.2.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-server-nodemanager;2.2.0!hadoop-yarn-server-nodemanager.jar (24ms)
[info] downloading https://repo1.maven.org/maven2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar ...
[info] 	[SUCCESSFUL ] org.fusesource.leveldbjni#leveldbjni-all;1.8!leveldbjni-all.jar(bundle) (45ms)
[info] downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.4.4/jackson-annotations-2.4.4.jar ...
[info] 	[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.4.4!jackson-annotations.jar(bundle) (8ms)
[info] downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.4.4/jackson-core-2.4.4.jar ...
[info] 	[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.4.4!jackson-core.jar(bundle) (14ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.curator#curator-framework;2.4.0!curator-framework.jar(bundle) (15ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.jar ...
[info] 	[SUCCESSFUL ] org.apache.zookeeper#zookeeper;3.4.5!zookeeper.jar (34ms)
[info] downloading https://repo1.maven.org/maven2/com/google/guava/guava/14.0.1/guava-14.0.1.jar ...
[info] 	[SUCCESSFUL ] com.google.guava#guava;14.0.1!guava.jar(bundle) (86ms)
[info] downloading https://repo1.maven.org/maven2/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar ...
[info] 	[SUCCESSFUL ] org.apache.curator#curator-client;2.4.0!curator-client.jar(bundle) (13ms)
[info] downloading https://repo1.maven.org/maven2/jline/jline/0.9.94/jline-0.9.94.jar ...
[info] 	[SUCCESSFUL ] jline#jline;0.9.94!jline.jar (9ms)
[info] downloading https://repo1.maven.org/maven2/com/typesafe/akka/akka-actor_2.10/2.3.11/akka-actor_2.10-2.3.11.jar ...
[info] 	[SUCCESSFUL ] com.typesafe.akka#akka-actor_2.10;2.3.11!akka-actor_2.10.jar (108ms)
[info] downloading https://repo1.maven.org/maven2/io/netty/netty/3.8.0.Final/netty-3.8.0.Final.jar ...
[info] 	[SUCCESSFUL ] io.netty#netty;3.8.0.Final!netty.jar(bundle) (52ms)
[info] downloading https://repo1.maven.org/maven2/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-maths-1.2.2a.jar ...
[info] 	[SUCCESSFUL ] org.uncommons.maths#uncommons-maths;1.2.2a!uncommons-maths.jar (8ms)
[info] downloading https://repo1.maven.org/maven2/com/typesafe/config/1.2.1/config-1.2.1.jar ...
[info] 	[SUCCESSFUL ] com.typesafe#config;1.2.1!config.jar(bundle) (21ms)
[info] downloading https://repo1.maven.org/maven2/org/scala-lang/scalap/2.10.4/scalap-2.10.4.jar ...
[info] 	[SUCCESSFUL ] org.scala-lang#scalap;2.10.4!scalap.jar (38ms)
[info] downloading https://repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar ...
[info] 	[SUCCESSFUL ] org.scala-lang#scala-compiler;2.10.4!scala-compiler.jar (572ms)
[info] downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar ...
[info] 	[SUCCESSFUL ] org.scala-lang#scala-reflect;2.10.4!scala-reflect.jar (123ms)
[info] downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.4/commons-io-2.4.jar ...
[info] 	[SUCCESSFUL ] commons-io#commons-io;2.4!commons-io.jar (15ms)
[info] downloading https://repo1.maven.org/maven2/org/tachyonproject/tachyon-underfs-hdfs/0.8.2/tachyon-underfs-hdfs-0.8.2.jar ...
[info] 	[SUCCESSFUL ] org.tachyonproject#tachyon-underfs-hdfs;0.8.2!tachyon-underfs-hdfs.jar (6ms)
[info] downloading https://repo1.maven.org/maven2/org/tachyonproject/tachyon-underfs-s3/0.8.2/tachyon-underfs-s3-0.8.2.jar ...
[info] 	[SUCCESSFUL ] org.tachyonproject#tachyon-underfs-s3;0.8.2!tachyon-underfs-s3.jar (31ms)
[info] downloading https://repo1.maven.org/maven2/org/tachyonproject/tachyon-underfs-local/0.8.2/tachyon-underfs-local-0.8.2.jar ...
[info] 	[SUCCESSFUL ] org.tachyonproject#tachyon-underfs-local;0.8.2!tachyon-underfs-local.jar (7ms)
[info] downloading https://repo1.maven.org/maven2/org/scala-lang/jline/2.10.4/jline-2.10.4.jar ...
[info] 	[SUCCESSFUL ] org.scala-lang#jline;2.10.4!jline.jar (12ms)
[info] Done updating.
[info] Compiling 1 Scala source to /usr/local/spark/target/scala-2.10/classes...
[info] 'compiler-interface' not yet compiled for Scala 2.10.4. Compiling...
[info]   Compilation completed in 12.105 s
[info] Packaging /usr/local/spark/target/scala-2.10/simple-project_2.10-1.0.jar ...
[info] Done packaging.
[success] Total time: 42 s, completed Feb 13, 2017 1:42:08 AM
[root@spark4 spark]# ls -la
total 1452
drwxr-xr-x. 17  500  500    4096 Feb 13 01:41 .
drwxr-xr-x. 13 root root    4096 Feb 13 01:09 ..
drwxr-xr-x.  2  500  500    4096 Jun 21  2016 bin
-rw-r--r--.  1 root root     201 Feb 13 01:41 build.sbt
-rw-r--r--.  1  500  500 1343562 Jun 21  2016 CHANGES.txt
drwxr-xr-x.  2  500  500    4096 Feb 13 01:10 conf
drwxr-xr-x.  3  500  500    4096 Jun 21  2016 data
-rw-r--r--.  1 root root     692 Feb 13 01:32 derby.log
drwxr-xr-x.  3  500  500    4096 Jun 21  2016 ec2
drwxr-xr-x.  3  500  500    4096 Jun 21  2016 examples
drwxr-xr-x.  2  500  500    4096 Jun 21  2016 lib
-rw-r--r--.  1  500  500   17352 Jun 21  2016 LICENSE
drwxr-xr-x.  2  500  500    4096 Jun 21  2016 licenses
drwxr-xr-x.  2 root root    4096 Feb 13 01:26 logs
drwxr-xr-x.  5 root root    4096 Feb 13 01:32 metastore_db
-rw-r--r--.  1  500  500   23529 Jun 21  2016 NOTICE
drwxr-xr-x.  3 root root    4096 Feb 13 01:41 project
drwxr-xr-x.  6  500  500    4096 Jun 21  2016 python
drwxr-xr-x.  3  500  500    4096 Jun 21  2016 R
-rw-r--r--.  1  500  500    3359 Jun 21  2016 README.md
-rw-r--r--.  1  500  500     120 Jun 21  2016 RELEASE
drwxr-xr-x.  3  500  500    4096 Feb 13 01:28 sbin
-rw-r--r--.  1 root root     660 Feb 13 01:40 SimpleApp.scala
drwxr-xr-x.  5 root root    4096 Feb 13 01:41 target
drwxr-xr-x.  2 root root    4096 Feb 13 01:16 work
[root@spark4 spark]# cd target
[root@spark4 target]# ls -la
total 20
drwxr-xr-x.  5 root root 4096 Feb 13 01:41 .
drwxr-xr-x. 17  500  500 4096 Feb 13 01:41 ..
-rw-r--r--.  1 root root    0 Feb 13 01:40 .history
drwxr-xr-x.  5 root root 4096 Feb 13 01:41 resolution-cache
drwxr-xr-x.  3 root root 4096 Feb 13 01:42 scala-2.10
drwxr-xr-x.  4 root root 4096 Feb 13 01:41 streams
[root@spark4 target]# cd ..
[root@spark4 spark]# find target -iname "*.jar"
target/scala-2.10/simple-project_2.10-1.0.jar
[root@spark4 spark]# $SPARK_HOME/bin/spark-submit --class "SimpleApp" \
> --master spark://spark1:7077 \
> $(find target -iname "*.jar")
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/02/13 01:42:56 INFO SparkContext: Running Spark version 1.6.2
17/02/13 01:42:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/02/13 01:42:56 INFO SecurityManager: Changing view acls to: root
17/02/13 01:42:56 INFO SecurityManager: Changing modify acls to: root
17/02/13 01:42:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/02/13 01:42:57 INFO Utils: Successfully started service 'sparkDriver' on port 39841.
17/02/13 01:42:57 INFO Slf4jLogger: Slf4jLogger started
17/02/13 01:42:57 INFO Remoting: Starting remoting
17/02/13 01:42:57 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 38920.
17/02/13 01:42:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@169.53.128.114:38920]
17/02/13 01:42:57 INFO SparkEnv: Registering MapOutputTracker
17/02/13 01:42:57 INFO SparkEnv: Registering BlockManagerMaster
17/02/13 01:42:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-43cdc2fb-67ac-4353-8baf-954cd212c2f5
17/02/13 01:42:57 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/02/13 01:42:57 INFO SparkEnv: Registering OutputCommitCoordinator
17/02/13 01:42:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/02/13 01:42:58 INFO SparkUI: Started SparkUI at http://169.53.128.114:4040
17/02/13 01:42:58 INFO HttpFileServer: HTTP File server directory is /tmp/spark-1b3194ea-bc78-4566-8173-1949e7a152f2/httpd-8c2c8ef5-bc8d-4dec-b5e6-7a72fa215688
17/02/13 01:42:58 INFO HttpServer: Starting HTTP Server
17/02/13 01:42:58 INFO Utils: Successfully started service 'HTTP file server' on port 42363.
17/02/13 01:42:58 INFO SparkContext: Added JAR file:/usr/local/spark/target/scala-2.10/simple-project_2.10-1.0.jar at http://169.53.128.114:42363/jars/simple-project_2.10-1.0.jar with timestamp 1486971778053
17/02/13 01:42:58 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1:7077...
17/02/13 01:42:58 WARN AppClient$ClientEndpoint: Failed to connect to master spark1:7077
java.io.IOException: Failed to connect to spark1/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
17/02/13 01:43:18 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1:7077...
17/02/13 01:43:18 WARN AppClient$ClientEndpoint: Failed to connect to master spark1:7077
java.io.IOException: Failed to connect to spark1/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
17/02/13 01:43:38 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1:7077...
17/02/13 01:43:38 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1:7077...
17/02/13 01:43:38 WARN AppClient$ClientEndpoint: Failed to connect to master spark1:7077
java.io.IOException: Failed to connect to spark1/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
^C17/02/13 01:43:46 INFO DiskBlockManager: Shutdown hook called
17/02/13 01:43:46 INFO ShutdownHookManager: Shutdown hook called
17/02/13 01:43:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b3194ea-bc78-4566-8173-1949e7a152f2/httpd-8c2c8ef5-bc8d-4dec-b5e6-7a72fa215688
17/02/13 01:43:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b3194ea-bc78-4566-8173-1949e7a152f2/userFiles-6ca41e94-95a2-4efc-b538-1c28b577d349
17/02/13 01:43:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b3194ea-bc78-4566-8173-1949e7a152f2
[root@spark4 spark]# 
[root@spark4 spark]# 
[root@spark4 spark]# 
[root@spark4 spark]# ping spark1
PING spark1.shankar.net (169.53.128.114) 56(84) bytes of data.
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=1 ttl=64 time=0.019 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=2 ttl=64 time=0.034 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=3 ttl=64 time=0.040 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=4 ttl=64 time=0.034 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=5 ttl=64 time=0.038 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=6 ttl=64 time=0.043 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=7 ttl=64 time=0.039 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=8 ttl=64 time=0.042 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=9 ttl=64 time=0.039 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=10 ttl=64 time=0.046 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=11 ttl=64 time=0.040 ms
64 bytes from spark1.shankar.net (169.53.128.114): icmp_seq=12 ttl=64 time=0.038 ms
^C
--- spark1.shankar.net ping statistics ---
12 packets transmitted, 12 received, 0% packet loss, time 10999ms
rtt min/avg/max/mdev = 0.019/0.037/0.046/0.009 ms
[root@spark4 spark]# $SPARK_HOME/bin/spark-submit --class "SimpleApp" --master spark://spark4:7077 $(find target -iname "*.jar")
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/02/13 01:44:21 INFO SparkContext: Running Spark version 1.6.2
17/02/13 01:44:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/02/13 01:44:21 INFO SecurityManager: Changing view acls to: root
17/02/13 01:44:21 INFO SecurityManager: Changing modify acls to: root
17/02/13 01:44:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/02/13 01:44:22 INFO Utils: Successfully started service 'sparkDriver' on port 36756.
17/02/13 01:44:22 INFO Slf4jLogger: Slf4jLogger started
17/02/13 01:44:22 INFO Remoting: Starting remoting
17/02/13 01:44:22 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 35130.
17/02/13 01:44:22 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@169.53.128.114:35130]
17/02/13 01:44:22 INFO SparkEnv: Registering MapOutputTracker
17/02/13 01:44:22 INFO SparkEnv: Registering BlockManagerMaster
17/02/13 01:44:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-466b79be-ad4c-49c3-99b6-9d35c7612bc4
17/02/13 01:44:22 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/02/13 01:44:22 INFO SparkEnv: Registering OutputCommitCoordinator
17/02/13 01:44:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/02/13 01:44:22 INFO SparkUI: Started SparkUI at http://169.53.128.114:4040
17/02/13 01:44:22 INFO HttpFileServer: HTTP File server directory is /tmp/spark-bd3f1989-f0e8-42bf-85f6-bcb088b014ab/httpd-ad9b3ef9-39a6-4966-9bb7-a067599ee44b
17/02/13 01:44:22 INFO HttpServer: Starting HTTP Server
17/02/13 01:44:22 INFO Utils: Successfully started service 'HTTP file server' on port 41295.
17/02/13 01:44:22 INFO SparkContext: Added JAR file:/usr/local/spark/target/scala-2.10/simple-project_2.10-1.0.jar at http://169.53.128.114:41295/jars/simple-project_2.10-1.0.jar with timestamp 1486971862993
17/02/13 01:44:23 INFO AppClient$ClientEndpoint: Connecting to master spark://spark4:7077...
17/02/13 01:44:43 INFO AppClient$ClientEndpoint: Connecting to master spark://spark4:7077...
17/02/13 01:45:03 INFO AppClient$ClientEndpoint: Connecting to master spark://spark4:7077...
17/02/13 01:45:03 INFO AppClient$ClientEndpoint: Connecting to master spark://spark4:7077...
^C17/02/13 01:45:10 INFO DiskBlockManager: Shutdown hook called
17/02/13 01:45:10 INFO ShutdownHookManager: Shutdown hook called
17/02/13 01:45:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd3f1989-f0e8-42bf-85f6-bcb088b014ab/httpd-ad9b3ef9-39a6-4966-9bb7-a067599ee44b
17/02/13 01:45:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd3f1989-f0e8-42bf-85f6-bcb088b014ab/userFiles-1c3f7b37-e039-4c52-b9c9-eb94d19c59bf
17/02/13 01:45:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd3f1989-f0e8-42bf-85f6-bcb088b014ab
[root@spark4 spark]# $SPARK_HOME/bin/spark-submit --class "SimpleApp" --master spark://spark1:7077 $(find target -iname "*.jar")
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/02/13 01:45:25 INFO SparkContext: Running Spark version 1.6.2
17/02/13 01:45:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/02/13 01:45:25 INFO SecurityManager: Changing view acls to: root
17/02/13 01:45:25 INFO SecurityManager: Changing modify acls to: root
17/02/13 01:45:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/02/13 01:45:25 INFO Utils: Successfully started service 'sparkDriver' on port 37955.
17/02/13 01:45:26 INFO Slf4jLogger: Slf4jLogger started
17/02/13 01:45:26 INFO Remoting: Starting remoting
17/02/13 01:45:26 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 38716.
17/02/13 01:45:26 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@169.53.128.114:38716]
17/02/13 01:45:26 INFO SparkEnv: Registering MapOutputTracker
17/02/13 01:45:26 INFO SparkEnv: Registering BlockManagerMaster
17/02/13 01:45:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-245bd4e6-a31d-4f3e-a00d-04e87d895161
17/02/13 01:45:26 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/02/13 01:45:26 INFO SparkEnv: Registering OutputCommitCoordinator
17/02/13 01:45:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/02/13 01:45:26 INFO SparkUI: Started SparkUI at http://169.53.128.114:4040
17/02/13 01:45:26 INFO HttpFileServer: HTTP File server directory is /tmp/spark-158e4851-7688-4470-96f9-786a742c08c0/httpd-e5f6b04f-c239-43bb-ad38-5c0709d3d8f4
17/02/13 01:45:26 INFO HttpServer: Starting HTTP Server
17/02/13 01:45:26 INFO Utils: Successfully started service 'HTTP file server' on port 43437.
17/02/13 01:45:26 INFO SparkContext: Added JAR file:/usr/local/spark/target/scala-2.10/simple-project_2.10-1.0.jar at http://169.53.128.114:43437/jars/simple-project_2.10-1.0.jar with timestamp 1486971926867
17/02/13 01:45:26 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1:7077...
17/02/13 01:45:26 WARN AppClient$ClientEndpoint: Failed to connect to master spark1:7077
java.io.IOException: Failed to connect to spark1/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
17/02/13 01:45:46 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1:7077...
17/02/13 01:45:46 WARN AppClient$ClientEndpoint: Failed to connect to master spark1:7077
java.io.IOException: Failed to connect to spark1/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
17/02/13 01:46:06 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1:7077...
17/02/13 01:46:06 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1:7077...
17/02/13 01:46:06 WARN AppClient$ClientEndpoint: Failed to connect to master spark1:7077
java.io.IOException: Failed to connect to spark1/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
17/02/13 01:46:26 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1:7077...
17/02/13 01:46:26 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1:7077...
17/02/13 01:46:26 ERROR SparkDeploySchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.
17/02/13 01:46:26 WARN SparkDeploySchedulerBackend: Application ID is not initialized yet.
17/02/13 01:46:26 WARN AppClient$ClientEndpoint: Failed to connect to master spark1:7077
java.io.IOException: Failed to connect to spark1/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
17/02/13 01:46:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46607.
17/02/13 01:46:26 INFO NettyBlockTransferService: Server created on 46607
17/02/13 01:46:26 INFO BlockManagerMaster: Trying to register BlockManager
17/02/13 01:46:26 INFO BlockManagerMasterEndpoint: Registering block manager 169.53.128.114:46607 with 511.1 MB RAM, BlockManagerId(driver, 169.53.128.114, 46607)
17/02/13 01:46:26 INFO BlockManagerMaster: Registered BlockManager
17/02/13 01:46:27 INFO SparkUI: Stopped Spark web UI at http://169.53.128.114:4040
17/02/13 01:46:27 INFO SparkDeploySchedulerBackend: Shutting down all executors
17/02/13 01:46:27 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
17/02/13 01:46:27 WARN AppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master
17/02/13 01:46:27 ERROR SparkContext: Error initializing SparkContext.
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:82)
SimpleApp$.main(SimpleApp.scala:10)
SimpleApp.main(SimpleApp.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.org$apache$spark$SparkContext$$assertNotStopped(SparkContext.scala:106)
	at org.apache.spark.SparkContext.getSchedulingMode(SparkContext.scala:1578)
	at org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:2179)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:579)
	at SimpleApp$.main(SimpleApp.scala:10)
	at SimpleApp.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/02/13 01:46:27 INFO SparkContext: SparkContext already stopped.
Exception in thread "main" java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:82)
SimpleApp$.main(SimpleApp.scala:10)
SimpleApp.main(SimpleApp.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.org$apache$spark$SparkContext$$assertNotStopped(SparkContext.scala:106)
	at org.apache.spark.SparkContext.getSchedulingMode(SparkContext.scala:1578)
	at org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:2179)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:579)
	at SimpleApp$.main(SimpleApp.scala:10)
	at SimpleApp.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/02/13 01:46:27 INFO DiskBlockManager: Shutdown hook called
17/02/13 01:46:27 INFO ShutdownHookManager: Shutdown hook called
17/02/13 01:46:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-158e4851-7688-4470-96f9-786a742c08c0/httpd-e5f6b04f-c239-43bb-ad38-5c0709d3d8f4
17/02/13 01:46:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-158e4851-7688-4470-96f9-786a742c08c0
17/02/13 01:46:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-158e4851-7688-4470-96f9-786a742c08c0/userFiles-a7eadd3d-b735-4c7f-b872-1992b4fdb83b
[root@spark4 spark]# find target -iname "*.jar"
target/scala-2.10/simple-project_2.10-1.0.jar
[root@spark4 spark]# ls -la
total 1452
drwxr-xr-x. 17  500  500    4096 Feb 13 01:41 .
drwxr-xr-x. 13 root root    4096 Feb 13 01:09 ..
drwxr-xr-x.  2  500  500    4096 Jun 21  2016 bin
-rw-r--r--.  1 root root     201 Feb 13 01:41 build.sbt
-rw-r--r--.  1  500  500 1343562 Jun 21  2016 CHANGES.txt
drwxr-xr-x.  2  500  500    4096 Feb 13 01:10 conf
drwxr-xr-x.  3  500  500    4096 Jun 21  2016 data
-rw-r--r--.  1 root root     692 Feb 13 01:32 derby.log
drwxr-xr-x.  3  500  500    4096 Jun 21  2016 ec2
drwxr-xr-x.  3  500  500    4096 Jun 21  2016 examples
drwxr-xr-x.  2  500  500    4096 Jun 21  2016 lib
-rw-r--r--.  1  500  500   17352 Jun 21  2016 LICENSE
drwxr-xr-x.  2  500  500    4096 Jun 21  2016 licenses
drwxr-xr-x.  2 root root    4096 Feb 13 01:26 logs
drwxr-xr-x.  5 root root    4096 Feb 13 01:32 metastore_db
-rw-r--r--.  1  500  500   23529 Jun 21  2016 NOTICE
drwxr-xr-x.  3 root root    4096 Feb 13 01:41 project
drwxr-xr-x.  6  500  500    4096 Jun 21  2016 python
drwxr-xr-x.  3  500  500    4096 Jun 21  2016 R
-rw-r--r--.  1  500  500    3359 Jun 21  2016 README.md
-rw-r--r--.  1  500  500     120 Jun 21  2016 RELEASE
drwxr-xr-x.  3  500  500    4096 Feb 13 01:28 sbin
-rw-r--r--.  1 root root     660 Feb 13 01:40 SimpleApp.scala
drwxr-xr-x.  5 root root    4096 Feb 13 01:41 target
drwxr-xr-x.  2 root root    4096 Feb 13 01:16 work
[root@spark4 spark]# cp SimpleApp.scala SimpleApp
[root@spark4 spark]# hostname
spark1.shankar.net
[root@spark4 spark]# $SPARK_HOME/bin/spark-submit --class "SimpleApp" --master spark://spark1.shankar.net:7077 $(find target -iname "*.jar")
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/02/13 01:48:44 INFO SparkContext: Running Spark version 1.6.2
17/02/13 01:48:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/02/13 01:48:44 INFO SecurityManager: Changing view acls to: root
17/02/13 01:48:44 INFO SecurityManager: Changing modify acls to: root
17/02/13 01:48:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/02/13 01:48:45 INFO Utils: Successfully started service 'sparkDriver' on port 45930.
17/02/13 01:48:45 INFO Slf4jLogger: Slf4jLogger started
17/02/13 01:48:45 INFO Remoting: Starting remoting
17/02/13 01:48:45 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@169.53.128.114:42184]
17/02/13 01:48:45 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 42184.
17/02/13 01:48:45 INFO SparkEnv: Registering MapOutputTracker
17/02/13 01:48:45 INFO SparkEnv: Registering BlockManagerMaster
17/02/13 01:48:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-05212400-fe36-4653-a826-ae8b009498ce
17/02/13 01:48:45 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/02/13 01:48:45 INFO SparkEnv: Registering OutputCommitCoordinator
17/02/13 01:48:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/02/13 01:48:46 INFO SparkUI: Started SparkUI at http://169.53.128.114:4040
17/02/13 01:48:46 INFO HttpFileServer: HTTP File server directory is /tmp/spark-8554babd-f334-4c52-ab21-8fabc266b91d/httpd-aa5ff60b-2a55-43e1-840f-f04f5023e8c3
17/02/13 01:48:46 INFO HttpServer: Starting HTTP Server
17/02/13 01:48:46 INFO Utils: Successfully started service 'HTTP file server' on port 38379.
17/02/13 01:48:46 INFO SparkContext: Added JAR file:/usr/local/spark/target/scala-2.10/simple-project_2.10-1.0.jar at http://169.53.128.114:38379/jars/simple-project_2.10-1.0.jar with timestamp 1486972126220
17/02/13 01:48:46 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1.shankar.net:7077...
17/02/13 01:48:46 WARN AppClient$ClientEndpoint: Failed to connect to master spark1.shankar.net:7077
java.io.IOException: Failed to connect to spark1.shankar.net/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1.shankar.net/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
17/02/13 01:49:06 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1.shankar.net:7077...
17/02/13 01:49:06 WARN AppClient$ClientEndpoint: Failed to connect to master spark1.shankar.net:7077
java.io.IOException: Failed to connect to spark1.shankar.net/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1.shankar.net/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
17/02/13 01:49:26 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1.shankar.net:7077...
17/02/13 01:49:26 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1.shankar.net:7077...
17/02/13 01:49:26 WARN AppClient$ClientEndpoint: Failed to connect to master spark1.shankar.net:7077
java.io.IOException: Failed to connect to spark1.shankar.net/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1.shankar.net/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
17/02/13 01:49:46 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1.shankar.net:7077...
17/02/13 01:49:46 ERROR SparkDeploySchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.
17/02/13 01:49:46 WARN SparkDeploySchedulerBackend: Application ID is not initialized yet.
17/02/13 01:49:46 INFO AppClient$ClientEndpoint: Connecting to master spark://spark1.shankar.net:7077...
17/02/13 01:49:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33026.
17/02/13 01:49:46 INFO NettyBlockTransferService: Server created on 33026
17/02/13 01:49:46 INFO BlockManagerMaster: Trying to register BlockManager
17/02/13 01:49:46 INFO BlockManagerMasterEndpoint: Registering block manager 169.53.128.114:33026 with 511.1 MB RAM, BlockManagerId(driver, 169.53.128.114, 33026)
17/02/13 01:49:46 INFO BlockManagerMaster: Registered BlockManager
17/02/13 01:49:46 WARN AppClient$ClientEndpoint: Failed to connect to master spark1.shankar.net:7077
java.io.IOException: Failed to connect to spark1.shankar.net/169.53.128.114:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: spark1.shankar.net/169.53.128.114:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
17/02/13 01:49:46 INFO SparkUI: Stopped Spark web UI at http://169.53.128.114:4040
17/02/13 01:49:46 INFO SparkDeploySchedulerBackend: Shutting down all executors
17/02/13 01:49:46 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
17/02/13 01:49:46 WARN AppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master
17/02/13 01:49:46 ERROR SparkContext: Error initializing SparkContext.
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:82)
SimpleApp$.main(SimpleApp.scala:10)
SimpleApp.main(SimpleApp.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.org$apache$spark$SparkContext$$assertNotStopped(SparkContext.scala:106)
	at org.apache.spark.SparkContext.getSchedulingMode(SparkContext.scala:1578)
	at org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:2179)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:579)
	at SimpleApp$.main(SimpleApp.scala:10)
	at SimpleApp.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/02/13 01:49:46 INFO SparkContext: SparkContext already stopped.
Exception in thread "main" java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:82)
SimpleApp$.main(SimpleApp.scala:10)
SimpleApp.main(SimpleApp.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.org$apache$spark$SparkContext$$assertNotStopped(SparkContext.scala:106)
	at org.apache.spark.SparkContext.getSchedulingMode(SparkContext.scala:1578)
	at org.apache.spark.SparkContext.postEnvironmentUpdate(SparkContext.scala:2179)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:579)
	at SimpleApp$.main(SimpleApp.scala:10)
	at SimpleApp.main(SimpleApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/02/13 01:49:46 INFO DiskBlockManager: Shutdown hook called
17/02/13 01:49:46 INFO ShutdownHookManager: Shutdown hook called
17/02/13 01:49:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-8554babd-f334-4c52-ab21-8fabc266b91d/httpd-aa5ff60b-2a55-43e1-840f-f04f5023e8c3
17/02/13 01:49:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-8554babd-f334-4c52-ab21-8fabc266b91d/userFiles-3030971d-8942-43b9-ac65-5a86ce7c8b62
17/02/13 01:49:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-8554babd-f334-4c52-ab21-8fabc266b91d
[root@spark4 spark]# ping 169.53.128.114
PING 169.53.128.114 (169.53.128.114) 56(84) bytes of data.
64 bytes from 169.53.128.114: icmp_seq=1 ttl=64 time=0.018 ms
64 bytes from 169.53.128.114: icmp_seq=2 ttl=64 time=0.044 ms
64 bytes from 169.53.128.114: icmp_seq=3 ttl=64 time=0.050 ms
64 bytes from 169.53.128.114: icmp_seq=4 ttl=64 time=0.037 ms
^C
--- 169.53.128.114 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 2999ms
rtt min/avg/max/mdev = 0.018/0.037/0.050/0.012 ms
[root@spark4 spark]# 

ooo[root@spark1 spark]# 
[root@spark1 spark]# $SPARK_HOME/bin/spark-submit --class "SimpleApp" --master local[4] $(find target -iname "*.jar")
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/02/14 00:15:30 INFO SparkContext: Running Spark version 1.6.2
17/02/14 00:15:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/02/14 00:15:30 INFO SecurityManager: Changing view acls to: root
17/02/14 00:15:30 INFO SecurityManager: Changing modify acls to: root
17/02/14 00:15:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/02/14 00:15:31 INFO Utils: Successfully started service 'sparkDriver' on port 33714.
17/02/14 00:15:31 INFO Slf4jLogger: Slf4jLogger started
17/02/14 00:15:31 INFO Remoting: Starting remoting
17/02/14 00:15:31 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 43633.
17/02/14 00:15:31 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@169.53.128.114:43633]
17/02/14 00:15:31 INFO SparkEnv: Registering MapOutputTracker
17/02/14 00:15:31 INFO SparkEnv: Registering BlockManagerMaster
17/02/14 00:15:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b95fa9a2-4a9e-4066-8945-0e8feba9868d
17/02/14 00:15:31 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/02/14 00:15:32 INFO SparkEnv: Registering OutputCommitCoordinator
17/02/14 00:15:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/02/14 00:15:32 INFO SparkUI: Started SparkUI at http://169.53.128.114:4040
17/02/14 00:15:32 INFO HttpFileServer: HTTP File server directory is /tmp/spark-827a910f-13b1-4ee8-bc0a-0ca1a88174db/httpd-972c6c64-b754-4385-a2ef-33ce10df519d
17/02/14 00:15:32 INFO HttpServer: Starting HTTP Server
17/02/14 00:15:32 INFO Utils: Successfully started service 'HTTP file server' on port 45911.
17/02/14 00:15:32 INFO SparkContext: Added JAR file:/usr/local/spark/target/scala-2.10/simple-project_2.10-1.0.jar at http://169.53.128.114:45911/jars/simple-project_2.10-1.0.jar with timestamp 1487052932231
17/02/14 00:15:32 INFO Executor: Starting executor ID driver on host localhost
17/02/14 00:15:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45114.
17/02/14 00:15:32 INFO NettyBlockTransferService: Server created on 45114
17/02/14 00:15:32 INFO BlockManagerMaster: Trying to register BlockManager
17/02/14 00:15:32 INFO BlockManagerMasterEndpoint: Registering block manager localhost:45114 with 511.1 MB RAM, BlockManagerId(driver, localhost, 45114)
17/02/14 00:15:32 INFO BlockManagerMaster: Registered BlockManager
17/02/14 00:15:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.4 KB, free 127.4 KB)
17/02/14 00:15:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.9 KB, free 141.3 KB)
17/02/14 00:15:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:45114 (size: 13.9 KB, free: 511.1 MB)
17/02/14 00:15:33 INFO SparkContext: Created broadcast 0 from textFile at SimpleApp.scala:11
17/02/14 00:15:33 INFO FileInputFormat: Total input paths to process : 1
17/02/14 00:15:33 INFO SparkContext: Starting job: count at SimpleApp.scala:12
17/02/14 00:15:33 INFO DAGScheduler: Got job 0 (count at SimpleApp.scala:12) with 2 output partitions
17/02/14 00:15:33 INFO DAGScheduler: Final stage: ResultStage 0 (count at SimpleApp.scala:12)
17/02/14 00:15:33 INFO DAGScheduler: Parents of final stage: List()
17/02/14 00:15:33 INFO DAGScheduler: Missing parents: List()
17/02/14 00:15:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at SimpleApp.scala:12), which has no missing parents
17/02/14 00:15:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 144.5 KB)
17/02/14 00:15:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1888.0 B, free 146.3 KB)
17/02/14 00:15:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:45114 (size: 1888.0 B, free: 511.1 MB)
17/02/14 00:15:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
17/02/14 00:15:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at SimpleApp.scala:12)
17/02/14 00:15:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
17/02/14 00:15:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2203 bytes)
17/02/14 00:15:33 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1,PROCESS_LOCAL, 2203 bytes)
17/02/14 00:15:33 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/02/14 00:15:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/02/14 00:15:33 INFO Executor: Fetching http://169.53.128.114:45911/jars/simple-project_2.10-1.0.jar with timestamp 1487052932231
17/02/14 00:15:33 INFO Utils: Fetching http://169.53.128.114:45911/jars/simple-project_2.10-1.0.jar to /tmp/spark-827a910f-13b1-4ee8-bc0a-0ca1a88174db/userFiles-5aa4ac9f-3c86-492c-a908-69bd184dcaa6/fetchFileTemp1191283134615531166.tmp
17/02/14 00:15:33 INFO Executor: Adding file:/tmp/spark-827a910f-13b1-4ee8-bc0a-0ca1a88174db/userFiles-5aa4ac9f-3c86-492c-a908-69bd184dcaa6/simple-project_2.10-1.0.jar to class loader
17/02/14 00:15:33 INFO CacheManager: Partition rdd_1_1 not found, computing it
17/02/14 00:15:33 INFO HadoopRDD: Input split: file:/usr/local/spark/README.md:1679+1680
17/02/14 00:15:33 INFO CacheManager: Partition rdd_1_0 not found, computing it
17/02/14 00:15:33 INFO HadoopRDD: Input split: file:/usr/local/spark/README.md:0+1679
17/02/14 00:15:33 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/02/14 00:15:33 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/02/14 00:15:33 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/02/14 00:15:33 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/02/14 00:15:33 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/02/14 00:15:33 INFO MemoryStore: Block rdd_1_1 stored as values in memory (estimated size 4.7 KB, free 151.0 KB)
17/02/14 00:15:33 INFO BlockManagerInfo: Added rdd_1_1 in memory on localhost:45114 (size: 4.7 KB, free: 511.1 MB)
17/02/14 00:15:33 INFO MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 5.4 KB, free 156.5 KB)
17/02/14 00:15:33 INFO BlockManagerInfo: Added rdd_1_0 in memory on localhost:45114 (size: 5.4 KB, free: 511.1 MB)
17/02/14 00:15:33 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2662 bytes result sent to driver
17/02/14 00:15:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2662 bytes result sent to driver
17/02/14 00:15:33 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 195 ms on localhost (1/2)
17/02/14 00:15:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 243 ms on localhost (2/2)
17/02/14 00:15:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/02/14 00:15:33 INFO DAGScheduler: ResultStage 0 (count at SimpleApp.scala:12) finished in 0.255 s
17/02/14 00:15:33 INFO DAGScheduler: Job 0 finished: count at SimpleApp.scala:12, took 0.361527 s
17/02/14 00:15:33 INFO SparkContext: Starting job: count at SimpleApp.scala:13
17/02/14 00:15:33 INFO DAGScheduler: Got job 1 (count at SimpleApp.scala:13) with 2 output partitions
17/02/14 00:15:33 INFO DAGScheduler: Final stage: ResultStage 1 (count at SimpleApp.scala:13)
17/02/14 00:15:33 INFO DAGScheduler: Parents of final stage: List()
17/02/14 00:15:33 INFO DAGScheduler: Missing parents: List()
17/02/14 00:15:33 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at SimpleApp.scala:13), which has no missing parents
17/02/14 00:15:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 159.6 KB)
17/02/14 00:15:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1890.0 B, free 161.4 KB)
17/02/14 00:15:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:45114 (size: 1890.0 B, free: 511.1 MB)
17/02/14 00:15:33 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/02/14 00:15:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at SimpleApp.scala:13)
17/02/14 00:15:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
17/02/14 00:15:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2203 bytes)
17/02/14 00:15:33 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1,PROCESS_LOCAL, 2203 bytes)
17/02/14 00:15:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
17/02/14 00:15:33 INFO BlockManager: Found block rdd_1_0 locally
17/02/14 00:15:33 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)
17/02/14 00:15:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2082 bytes result sent to driver
17/02/14 00:15:33 INFO BlockManager: Found block rdd_1_1 locally
17/02/14 00:15:33 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 2082 bytes result sent to driver
17/02/14 00:15:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 27 ms on localhost (1/2)
17/02/14 00:15:33 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 30 ms on localhost (2/2)
17/02/14 00:15:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/02/14 00:15:33 INFO DAGScheduler: ResultStage 1 (count at SimpleApp.scala:13) finished in 0.034 s
17/02/14 00:15:33 INFO DAGScheduler: Job 1 finished: count at SimpleApp.scala:13, took 0.051641 s
+++++++++++ Lines with a: 58, Lines with b: 26 ++++++++++
17/02/14 00:15:33 INFO SparkContext: Invoking stop() from shutdown hook
17/02/14 00:15:33 INFO SparkUI: Stopped Spark web UI at http://169.53.128.114:4040
17/02/14 00:15:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/02/14 00:15:33 INFO MemoryStore: MemoryStore cleared
17/02/14 00:15:33 INFO BlockManager: BlockManager stopped
17/02/14 00:15:33 INFO BlockManagerMaster: BlockManagerMaster stopped
17/02/14 00:15:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/02/14 00:15:33 INFO SparkContext: Successfully stopped SparkContext
17/02/14 00:15:33 INFO ShutdownHookManager: Shutdown hook called
17/02/14 00:15:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-827a910f-13b1-4ee8-bc0a-0ca1a88174db/httpd-972c6c64-b754-4385-a2ef-33ce10df519d
17/02/14 00:15:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-827a910f-13b1-4ee8-bc0a-0ca1a88174db
17/02/14 00:15:33 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/02/14 00:15:33 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
[root@spark1 spark]# 
